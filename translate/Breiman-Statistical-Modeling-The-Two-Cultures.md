# Statistical Modeling: The Two Cultures 
# 统计模型：两种文化
## Leo Breiman
Leo Breiman is Professor, Department of Statistics University of California, Berkeley, California 94720 4735 (e-maul: leo@stat.berkeley.edu).

Leo Breiman　是加州大学伯克利分校统计系教授 
电子邮件：leo@stat.berkeley.edu

## Abstract
## 摘要

There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools. 

在从数据到结论的过程中，有两种统计建模文化。第一种是数据模型，假设数据是通过给定的随机数据模型生成的。另一种是算法模型，将数据生成机制视为未知。一直以来，统计界几乎完全使用数据模型。这种情况造就了无关紧要的理论、有问题的结论，并使统计学家无法研究广阔、有趣的现实问题。算法建模，无论在理论上还是实践上，都在统计学之外的领域飞速发展。它即可以用于大型复杂的数据集，也可以用于小型数据集。在小型数据集的处理上，算法模型也比数据模型更准确，能产生更丰富的信息。如果我们的目标是让统计学能够使用数据来解决问题，那么我们需要摆脱对数据模型的完全依赖，采用更多样化的工具。

## 1. INTRODUCTION
## 1. 介绍


Statistics starts with data. Think of the data as being generated by a black box in which a vector of input variables $\mathbf{x}$ (independent variables) $\mathrm{go}$ in one side, and on the other side the response variables $\mathbf{y}$ come out. Inside the black box, nature functions to associate the predictor variables with the response variables, so the picture is like this:

统计始于数据。 将数据看作是由一个黑匣子生成的，其中一边（右边）是以输入自变量x构成的向量，另一边（左边）是响应变量y。在黑匣子内部，自然函数将预测变量（自变量）与响应变量（因变量）相关联，因此图片如下：

![](https://cdn.mathpix.com/cropped/0ff65c4b1d28187c8f7d0addfa01c783-01.jpg?height=77&width=299&top_left_y=839&top_left_x=200)

There are two goals in analyzing the data:

分析数据有两个目标：

Prediction. To be able to predict what the responses are going to be to future input variables; 

预测。 能够预测自变量在未来将会有什么样的响应；

Information. To extract some information about how nature is associating the response variables to the input variables.

信息。(以了解) 因变量和自变量之间的关联是否自然。

There are two different approaches toward these
实现这些目标有两种不同的方法：
###  The Data Modeling Culture　
### 数据建模文化

The analysis in this culture starts with assuming a stochastic data model for the inside of the black box. For example, a common data model is that data are generated by independent draws from：
response variables $=f$ (predictor variables,
random noise, parameters)

分析在数据建模文化中始于在黑匣子内部假设一个随机数据模型。 例如， 一个常见的数据模型指数据是通过独立抽取产生的：
因变量＝f(预测变量，随机扰乱项，参数)

The values of the parameters are estimated from the data and the model then used for information and/or prediction. Thus the black box is filled in like this:
参数的值是从数据和模型中估计出来的，然后用于信息和/或预测。 因此，黑匣子是这样填写的：

![](https://cdn.mathpix.com/cropped/0ff65c4b1d28187c8f7d0addfa01c783-01.jpg?height=77&width=301&top_left_y=735&top_left_x=746)

Model validation. Yes-no using goodness-of-fit tests and residual examination.

模型验证。使用拟合优度检验和残差检验来判断是否。

Estimated culture population. $98 \%$ of all statisticians.

估计人口文化，98%的统计学家。
###  The Algorithmic Modeling Culture
### 算法建模文化

The analysis in this culture considers the inside of the box complex and unknown. Their approach is to find a function $f(\mathbf{x})$-an algorithm that operates on $\mathbf{x}$ to predict the responses $\mathbf{y}$. Their black box looks like this:
在这种文化里的分析考虑的是盒子内部的复杂和未知。 他们的方法是找到一个函数 $f(\mathbf{x})$-一种运算法则对 x 进行操作来预测 y 结果的算法：

![](https://cdn.mathpix.com/cropped/0ff65c4b1d28187c8f7d0addfa01c783-01.jpg?height=143&width=353&top_left_y=1111&top_left_x=720)

Model validation. Measured by predictive accuracy. Estimated culture population. $2 \%$ of statisticians, many in other fields.

模型验证。通过预测的精确性来测量。估计人口文化。统计学家的$2\%$，很多在其他领域。

In this paper I will argue that the focus in the statistical community on data models has:

在这个文章里，我将争论统计学界对数据模型的关注有：

- Led to irrelevant theory and questionable scientific conclusions: - Kept statisticians from using more suitable algorithmic models;

- 导致不相关理论和问题的科学结论： - 阻止统计学家使用更合适的算法模型；

- Prevented statisticians from working on exciting new problems;

- 阻止统计学家处理令人兴奋的新问题；

I will also review some of the interesting new developments in algorithmic modeling in machine learning and look at applications to three data sets.

我还将回顾算法建模在机器学习方面的一些有趣新进展，并关注对三个数据集的应用。

## 2. ROAD MAP
## 2. 路线图

It may be revealing to understand how I became a member of the small second culture. After a seven year stint as an academic probabilist, I resigned and went into full-time free-lance consulting. After thirteen years of consulting I joined the Berkeley Statistics Department in 1980 and have been there since. My experiences as a consultant formed my views about algorithmic modeling. Section 3 describes two of the projects I worked on. These are given to show how my views grew from such problems.

理解我是如何成为两种文化的一员可能会有启迪作用。在做了7年的学术概率论学者之后，我辞职然后开始了全职的自由顾问生涯。在做了13年的咨询工作后，我于1980年加入了伯克利统计学院并一直在那里工作。我作为顾问的经历形成了我对算法建模的看法。第3部分描述了我参与的两个项目。这些都是为了说明我的观点是如何从这些问题中产生的。

When I returned to the university and began reading statistical journals, the research was distant from what I had done as a consultant. All articles begin and end with data models. My observations about published theoretical research in statistics are in Section 4 .

当我回到大学，开始阅读统计期刊时，这项研究与我作为顾问时所做的工作相去甚远。所有文章都以数据模型开始和结束。我对已发表的统计学理论研究的观察见第 4部分。

Data modeling has given the statistics field many successes in analyzing data and getting information about the mechanisms producing the data. But there is also misuse leading to questionable conclusions about the underlying mechanism. This is reviewed in Section 5. Following that is a discussion (Section 6) of how the commitment to data modeling has prevented statisticians from entering new scientific and commercial fields where the data being gathered is not suitable for analysis by data models.

数据建模使统计领域在分析数据和获取有关产生数据的机制的信息方面取得了许多成功。但也有滥用导致对潜在机制的可疑结论。这将在第5部分中回顾。接下来的讨论(第６部分)是关于投身数据建模是如何阻止统计学家进入新的科学和商业领域，在这些领域中收集的数据不适合通过数据模型进行分析。

In the past fifteen years, the growth in algorithmic modeling applications and methodology has been rapid. It has occurred largely outside statistics in a new community-often called machine learning -that is mostly young computer scientists (Section 7). The advances, particularly over the last five years, have been startling. Three of the most important changes in perception to be learned from these advances are described in Sections 8,9 , and 10 , and are associated with the following names:

在过去的15年里，算法建模应用和方法论的发展非常迅速。它主要发生在一个新的统计数据之外的群体——通常称为机器学习，这个群体主要是年轻计算机科学家(第7部分)。尤其是在过去5年里，这些进步令人吃惊。从这些进步中可以学到的三个最重要的观念变化在第 8、9 和 10 部分中都有描述，它们与以下名称相关:

Rashomon: the multiplicity of good models;
Rashomon:　好模型的多样性；

Occam: the conflict between simplicity and accuracy;
Occam: 简单与准确之间的冲突；

Bellman: dimensionality-curse or blessing?
Bellman:　维度－诅咒还是祝福？

Section 11 is titled "Information from a Black Box" and is important in showing that an algorithmic model can produce more and more reliable information about the structure of the relationship between inputs and outputs than data models. This is illustrated using two medical data sets and a genetic data set. A glossary at the end of the paper explains terms that not all statisticians may be familiar with.

第 11 节的标题是“来自黑匣子的信息”，它在说明算法模型比起数据模型能产出关于输入和输出关系结构的更可靠信息。这里用两个医学数据集和一个遗传数据集来说明。论文末尾的术语表解释了并非所有统计学家都熟悉的术语。

## 3. PROJECTS IN CONSULTING
## 3. 项目咨询

As a consultant 1 designed and helped supervise surveys for the Environmental Protection Agency (EPA) and the state and federal court systems. Controlled experiments were designed for the EPA, and I analyzed traffic data for the U.S. Department of Transportation and the California Transportation Department. Most of all, I worked on a diverse set of prediction projects. Here are some examples:

作为一名顾问，我为环境保护局(EPA)以及州和联邦法院系统设计并帮助监督调查。对照实验是为美国环保署设计的，我为美国交通部和加州交通部分析交通数据。最重要的是，我参与了一系列不同的预测项目。下面是一些例子:

Predicting next-day ozone levels.

预测第二天的臭氧水平。

Using mass spectra to identify halogen-containing compounds.

用质谱鉴定含卤素化合物。

Predicting the class of a ship from high altitude radar returns.

从高空雷达预测一艘船的等级。

Using sonar returns to predict the class of a submarine.

用声纳返回来预测潜艇的级别。

Identity of hand-sent Morse Code.

身份的手送莫尔斯电码。

Toxicity of chemicals.

化学物质的毒性。

On-line prediction of the cause of a freeway traffic breakdown.

高速公路交通故障原因的在线预测。

Speech recognition

语音识别

The sources of delay in criminal trials in state court systems.

州法院系统中刑事审判拖延的根源。

To understand the nature of these problems and the approaches taken to solve them, I give a fuller description of the first two on the list.

为了理解这些问题的本质和解决它们的方法，我对列表上的前两个问题进行了更全面的描述。

###  3.1 The Ozone Project
### 3.1 臭氧项目

In the mid- to late 1960 s ozone levels became a serious health problem in the Los Angeles Basin. Three different alert levels were established. At the highest, all government workers were directed not to drive to work, children were kept off playgrounds and outdoor exercise was discouraged.

20 世纪 60 年代中后期，洛杉矶盆地的臭氧水平成为一个严重的健康问题。设立了三个不同的警报级别。最高的规定是，所有政府工作人员不得开车上班，儿童不得在操场玩耍，也不鼓励户外运动。

The major source of ozone at that time was automobile tailpipe emissions. These rose into the low atmosphere and were trapped there by an inversion layer. A complex chemical reaction, aided by sunlight, cooked away and produced ozone two to three hours after the morning commute hours. The alert warnings were issued in the morning, but would be more effective if they could be issued 12 hours in advance. In the mid-1970s, the EPA funded a large effort to see if ozone levels could be accurately predicted 12 hours in advance.

当时臭氧的主要来源是汽车尾气排放。它们上升到低空大气中，并被逆温层困在那里。一种复杂的化学反应，在阳光的帮助下，在早上的通勤时间结束后的两到三个小时，就会蒸发并产生臭氧。预警是在早上发布的，但如果能提前12小时发布，效果会更好。在 20 世纪 70 年代中期，美国环保署出资进行了一项大型研究，以确定是否臭氧水平能被提前１２小时精确预测。

Commuting patterns in the Los Angeles Basin are regular, with the total variation in any given daylight hour varying only a few percent from one weekday to another. With the total amount of emissions about constant, the resulting ozone levels depend on the meteorology of the preceding days. A large data base was assembled consisting of lower and upper air measurements at U.S. weather stations as far away as Oregon and Arizona, together with hourly readings of surface temperature, humidity, and wind speed at the dozens of air pollution stations in the Basin and nearby areas.

洛杉矶盆地的通勤模式是有规律的，总变化在任何给定日光时间在工作日和工作日之间只相差几个百 分点。由于排放总量基本不变，由此产生的臭 氧水平取决于前几天的气象情况。一个庞大的数据库是由远在俄勒冈州和亚利桑那州的美国气象站的低空和高空的测量数据，以及盆地和附近地区几十个空气污染站每小时的地表温度、湿度和风速读数组成的。

Altogether, there were daily and hourly readings of over 450 meteorological variables for a period of seven years, with corresponding hourly values of ozone and other pollutants in the Basin. Let $\mathbf{x}$ be the predictor vector of meteorological variables on the $n$th day. There are more than 450 variables in $\mathbf{x}$ since information several days back is included. Let $y$ be the ozone level on the $(n+1)$ st day. Then the problem was to construct a function $f(\mathbf{x})$ such that for any future day and future predictor variables $\mathbf{x}$ for that day, $f(\mathbf{x})$ is an accurate predictor of the next day's ozone level $y$.

总而言之，在七年的时间里，有超过450个气象变量的每日和每小时读数，以及流域内臭氧和其他污染物的相应每小时读数。设 x 为第 n 天气象变量的预测向量。由于包含了几天前的信息，x 中有超过 450 个变量。设 y 为 $(n + 1)$天。问题是如何构造函数$f(\mathbf{x})$对于未来的任何一天,以及未来的预测变量 x, $f(\mathbf{x})$是对第二天臭氧水平 y 的准确预测。

To estimate predictive accuracy, the first five years of data were used as the training set. The last two years were set aside as a test set. The algorithmic modeling methods available in the pre1980 s decades seem primitive now. In this project large linear regressions were run, followed by variable selection. Quadratic terms in, and interactions among, the retained variables were added and variable selection used again to prune the equations. In the end, the project was a failure-the false alarm rate of the final predictor was too high. I have regrets that this project can't be revisited with the tools available today.

为了估计预测的准确性，我们使用前 5 年的数据作为处理集。过去的两年被作为一套测试。20 世纪 80 年代以前可用的算法建模方法现在看来似乎很原始。在这个项目中，进行了大的线性回归，然后进行了变量选择。在保留变量中加入二次项 和变量间的交互作用，再次使用变量选择对方程进行精简。最后，项目失败了——最终预测器的误报率太高。我很遗憾，这个项目不能用现有的工具重新访问。

###  3.2 The Chlorine Project
### 3.2 氯项目

The EPA samples thousands of compounds a year and tries to determine their potential toxicity. In the mid-1970s, the standard procedure was to measure the mass spectra of the compound and to try to determine its chemical structure from its mass spectra.

环保署每年对数千种化合物进行取样并试图确定它们的潜在毒性。在 20 世纪 70 年代中期，标准程序是测量化合物的质谱并试图从质谱中确定其化学结构。

Measuring the mass spectra is fast and cheap. But the determination of chemical structure from the mass spectra requires a painstaking examination by a trained chemist. The cost and availability of enough chemists to analyze all of the mass spectra produced daunted the EPA. Many toxic compounds contain halogens. So the EPA funded a project to determine if the presence of chlorine in a compound could be reliably predicted from its mass spectra.

质谱的测量既快速又便宜。但是，要从质谱中确定化学结构需要经过训练的化学家进行艰苦的检查。成本太高，而且需要有充分的化学家来分析所有产生的质谱，这让 EPA 望而却步。许多有毒化合物含有卤素。因此，环保局资助了一个项目，以确定是否可以从质谱中可靠地预测化合物中氯的存在。

Mass spectra are produced by bombarding the compound with ions in the presence of a magnetic field. The molecules of the compound split and the lighter fragments are bent more by the magnetic field than the heavier. Then the fragments hit an absorbing strip, with the position of the fragment on the strip determined by the molecular weight of the fragment. The intensity of the exposure at that position measures the frequency of the fragment. The resultant mass spectra has numbers reflecting frequencies of fragments from molecular weight 1 up to the molecular weight of the original compound. The peaks correspond to frequent fragments and there are many zeroes. The available data base consisted of the known chemical structure and mass spectra of 30,000 compounds.

质谱是在磁场的作用下用离子轰击化合物而产生的字段。化合物的分子分裂，较轻的碎片比较重的碎片更容易被磁场弯曲。然后碎片撞击吸收带，碎片在吸收带上的位置由碎片的分子量决定。在那个位置的曝光强度测量了碎片的频率。合成的质谱数字反映了从分子量 1 到原始化合物分子量的碎片的频率。这些峰值对应于频繁的碎片，并且有许多零。现有的数据库包括 3 万种化合物的已知化学结构和质谱。

The mass spectrum predictor vector $\mathbf{x}$ is of variable dimensionality. Molecular weight in the data base varied from 30 to over 10,000 . The variable to be predicted is

质谱预测向量 x 是变维的。数据库中的分子量从 30 到超过 10,000 不等。要预测的变量是

$$
\begin{aligned}
&y=1: \text { contains chlorine, } \\
&y=2 \text { : does not contain chlorine. }
\end{aligned}
$$

Y = 1:含氯，

Y = 2:不含氯。


The problem is to construct a function $f(\mathbf{x})$ that is an accurate predictor of $y$ where $\mathbf{x}$ is the mass spectrum of the compound.

问题是构造一个函数$f(x)$是对 y 的准确预测其中 x 是化合物的质谱。

To measure predictive accuracy the data set was randomly divided into a 25,000 member training set and a 5,000 member test set. Linear discriminant analysis was tried, then quadratic discriminant analysis. These were difficult to adapt to the variable dimensionality. By this time I was thinking about decision trees. The hallmarks of chlorine in mass spectra were researched. This domain knowledge was incorporated into the decision tree algorithm by the design of the set of 1,500 yes-no questions that could be applied to a mass spectra of any dimensionality. The result was a decision tree that gave $95 \%$ accuracy on both chlorines and nonchlorines (see Breiman, Friedman, Olshen and Stone, 1984).

为了测量预测的准确性，将数据集随机分为一个25000成员的处理集和一个 5000 成员的测试集。先进行线性判别分析，然后进行二次判别分析。这些都很难适应可变的维度。这时我想到了决策树。研究了质谱中氯的特征。这个领域的知识通过设计一套 1500 个是-否问题，被整合到决策树算法中，这些问题可以应用于任何维度的质谱。其结果是一个决策树，对氯和非氯都给出了 95% 的准确性(见 Breiman, Friedman, Olshen 和 Stone, 1984)。

###  3.3 Perceptions on Statistical Analysis
### 3.3 对统计分析的看法

As I left consulting to go back to the university, these were the perceptions I had about working with data to find answers to problems:

当我离开咨询业回到大学时， 我对用数据来寻找问题答案的看法如下:

(a) Focus on finding a good solution-that's what consultants get paid for.

(a)专注于寻找一个好的解决方案——这就是顾问们获得报酬的原因。

(b) Live with the data before you plunge into modeling.

(b)在开始建模之前，先了解数据。

(c) Search for a model that gives a good solution, either algorithmic or data.

(c)寻找一个能提供良好解决方案的模型，无论是算法还是数据。

(d) Predictive accuracy on test sets is the criterion for how good the model is.

(d)对测试集的预测准确性是衡量模型好坏的标准。

(e) Computers are an indispensable partner. 

(e)电脑是不可或缺的伙伴。


## 4. RETURN TO THE UNIVERSITY
## 4. 重返大学

I had one tip about what research in the university was like. A friend of mine, a prominent statistician from the Berkeley Statistics Department, visited me in Los Angeles in the late $1970 \mathrm{~s}$. After I described the decision tree method to him, his first question was, "What's the model for the data?"

关于大学里的研究，我有一个小建议。上世纪70年代末，我的一个朋友在洛杉矶拜访了我，他是伯克利统计学院(Berkeley Statistics Department)的一位杰出统计学家。在我向他描述决策树方法 后，他的第一个问题是 “数据的模型是什么?”

###  4.1 Statistical Research
### 4.1 统计研究

Upon my return, I started reading the Annals of Statistics, the flagship journal of theoretical statistics, and was bemused. Every article started with
Assume that the data are generated by the following model: ...
回国后， 我开始阅读理论统计学的旗舰期刊《统计年鉴》，感到很困惑。每篇文章都以假设数据是由以下模型生成的开始...

followed by mathematics exploring inference, hypothesis testing and asymptotics. There is a wide spectrum of opinion regarding the usefulness of the theory published in the Annals of Statistics to the field of statistics as a science that deals with data. I am at the very low end of the spectrum. Still, there have been some gems that have combined nice theory and significant applications. An example is wavelet theory. Even in applications, data models are universal. For instance, in the Journal of the American Statistical Association (JASA), virtually every article contains a statement of the form:
Assume that the data are generated by the following model: ...
接下来是数学探索推理，假设检验和渐近。对于发表在《统计年鉴》上的这一理论是否对统计学领域作为一门处理数据的科学有用，有广泛的意见。我处在这个范围的最低端。尽管如此，还是有一些结合了良好的理论和重要的应用的宝石。小波理论就是一个例子。即使在应用程序中，数据模型也是通用的。例如《美国统计协会杂志》 (Journal of American Statistical Association (JASA) ， 几乎每一篇文章都包含这样的语句:
假设数据是由以下模型生成的...

I am deeply troubled by the current and past use of data models in applications, where quantitative conclusions are drawn and perhaps policy decisions made.

我对当前和过去在应用程序中使用数据模型深感困扰，在这些应用程序中得出定量结论并可能做出政策决策。

## 5. THE USE OF DATA MODELS
## 5.数据模型的使用

Statisticians in applied research consider data modeling as the template for statistical analysis: Faced with an applied problem, think of a data model. This enterprise has at its heart the belief that a statistician, by imagination and by looking at the data, can invent a reasonably good parametric class of models for a complex mechanism devised by nature. Then parameters are estimated and conclusions are drawn. But when a model is fit to data to draw quantitative conclusions:

应用研究中的统计学家将数据建模视为统计分析的模板:面对应用问题时，考虑数据模型。这个企业的核心信念是统计学家通过想象和查看数据，可以为自然设计的复杂机制发明一个相当好的参数化模型。然后对参数进行估计并得出结论。但当一个模型适合于数据并得出定量结论时:

- The conclusions are about the model's mechanism, and not about nature's mechanism.

•结论是关于模型的机制，而不是关于自然的机制。

It follows that:

它遵循:

- If the model is a poor emulation of nature, the conclusions may be wrong. These truisms have often been ignored in the enthusiasm for fitting data models. A few decades ago, the commitment to data models was such that even simple precautions such as residual analysis or goodness-of-fit tests were not used. The belief in the infallibility of data models was almost religious. It is a strange phenomenon-once a model is made, then it becomes truth and the conclusions from it are infallible.

如果模型是对自然的拙劣模仿，那么结论可能是错误的。在对拟合数据模型的热情中让这些真理常常被忽视。几十年前，由于对数据模型的重视，甚至连残差分析或拟合优度检验等简单的预防措施都不被使用。相信数据模型的正确性几乎是一种宗教信仰。这是一种奇怪的现象——一旦一个模型被建立起来，它就变成了真理，从它得到的结论是极准确的。

###  5.1 An Example
### 5.1 一个例子

I illustrate with a famous (also infamous) example: assume the data is generated by independent draws from the model

我用一个著名的(也是臭名昭著的)例子来说明: 假设数据是由来自模型的独立绘图生成的

$$
y=b_{0}+\sum_{1}^{M} b_{m} x_{m}+\varepsilon
$$

where the coefficients $\left\{b_{m}\right\}$ are to be estimated, $\varepsilon$ is $N\left(0, \sigma^{2}\right)$ and $\sigma^{2}$ is to be estimated. Given that the data is generated this way, elegant tests of hypotheses, confidence intervals, distributions of the residual sum-of-squares and asymptotics can be derived. This made the model attractive in terms of the mathematics involved. This theory was used both by academic statisticians and others to derive significance levels for coefficients on the basis of model (R), with little consideration as to whether the data on hand could have been generated by a linear model. Hundreds, perhaps thousands of articles were published claiming proof of something or other because the coefficient was significant at the $5 \%$ level.

系数$\left\{b_{m}\right\}$被估算，$\varepsilon$是$N\left(0, \sigma^{2}\right)$且$\sigma^{2}$是需要被估算的。假设数据是以这种方式生成的，那么就可以推导出对假设、置信区间、残差平方和分布和渐近的精确检验。 这使得该模型在涉及的数学方面具有吸引力。这一理论被学术统计学家和其他人使用来推导基于模型(R)的系数的显著性水平，很少考虑到手头的数据是否可以由一个线性模型产生。成百上千的文章发表声称证明了什么因为系数在5%的水平上很重要。

Goodness-of-fit was demonstrated mostly by giving the value of the multiple correlation coefficient $R^{2}$ which was often closer to zero than one and which could be over inflated by the use of too many parameters. Besides computing $R^{2}$, nothing else was done to see if the observational data could have been generated by model (R). For instance, a study was done several decades ago by a well-known member of a university statistics department to assess whether there was gender discrimination in the salaries of the faculty. All personnel files were examined and a data base set up which consisted of salary as the response variable and 25 other variables which characterized academic performance; that is, papers published, quality of journals published in, teaching record, evaluations, etc. Gender appears as a binary predictor variable.

拟合优度主要是通过给出多重相关系数R2的值来证明的，R2往往更接近于0而不是 1，而且由于使用了太多的参数，R2可能会过度膨胀。除了计算R2,什么是观测数据是否可以由模型(R)。例如,一项研究是几十年前由知名的大学统计部门评估是否存在性别歧视在教师的工资。所有的人事档案都进行了检查，并建立了一个数据库， 该数据库由工资作为响应变量和其他25个描述学业成绩的变量组成;即发表论文、发表期刊质量、教学 记录、评价等。性别是一个二元预测变量。

A linear regression was carried out on the data and the gender coefficient was significant at the $5 \%$ level. That this was strong evidence of sex discrimination was accepted as gospel. The design of the study raises issues that enter before the consideration of a model-Can the data gathered answer the question posed? Is inference justified when your sample is the entire population? Should a data model be used? The deficiencies in analysis occurred because the focus was on the model and not on the problem.

对数据进行线性回归，性别系数在5%水平显著。这是性别歧视的有力证据，这被当作真理接受。这项研究的设计提出了在考虑模型之前进入的问题——数据能否收集回答这个问题? 当你的样本是整个总体时，推理是否合理?应该使用数据模型吗?分析中出现的缺陷是因为关注的是模型而不是问题。

The linear regression model led to many erroneous conclusions that appeared in journal articles waving the $5 \%$ significance level without knowing whether the model fit the data. Nowadays, I think most statisticians will agree that this is a suspect way to arrive at conclusions. At the time, there were few objections from the statistical profession about the fairy-tale aspect of the procedure, But, hidden in an elementary textbook, Mosteller and Tukey (1977) discuss many of the fallacies possible in regression and write "The whole area of guided regression is fraught with intellectual, statistical, computational, and subject matter difficulties.”

线性回归模型导致许多错误的结论出现在期刊文章中，这些文章5%的显著性水平，而不知道模型是否适合数据。如今，我想大多数统计学家都会同意，这种得出结论的方式令人怀疑。当时,几乎没有反对关于童话的统计专业方面的程序,但是,隐藏在一个基础教材,Mosteller 和Tukey(1977)讨论的许多谬误可能回归写作“整个地区的引导回归充满了知识,统计,计算,以及主题困难。”

Even currently, there are only rare published critiques of the uncritical use of data models. One of the few is David Freedman, who examines the use of regression models (1994); the use of path models (1987) and data modeling (1991, 1995). The analysis in these papers is incisive.

即使在目前，对数据模型不加批判的使用也只有很少的公开批评。其中之一是大卫·弗里德曼，他研究了回归模型的使用(1994年);路径模型(1987)和数据模型(1991,1995)的使用。这些论文的分析是敏锐的。

###  5.2 Problems in Current Data Modeling
### 5.2 当前数据建模存在的问题

Current applied practice is to check the data model fit using goodness-of-fit tests and residual analysis. At one point, some years ago, I set up a simulated regression problem in seven dimensions with a controlled amount of nonlinearity. Standard tests of goodness-of-fit did not reject linearity until the nonlinearity was extreme. Recent theory supports this conclusion. Work by Bickel, Ritov and Stoker (2001) shows that goodness-of-fit tests have very little power unless the direction of the alternative is precisely specified. The implication is that omnibus goodness-of-fit tests, which test in many directions simultaneously, have little power, and will not reject until the lack of fit is extreme.

目前的应用实践是通过拟合优度检验和残差分析来检验数据模型的拟合。几年前， 我建立了一个模拟的七维回归问题，其中包含一些受控的非线性。标准的拟合优度测试直到非线性达到极端时才拒绝线性。最近的理论支持这一结论。Bickel, Ritov 和 Stoker(2001)的研究表明，除非明确指定替代的方向，否则拟合度检验的效力很小。这意味着，在多个方向同时进行测试的综合拟合优度测试的效力很小，除非缺乏拟合的情况非常严重，否则不会拒绝。

Furthermore, if the model is tinkered with on the basis of the data, that is, if variables are deleted or nonlinear combinations of the variables added, then goodness-of-fit tests are not applicable. Residual analysis is similarly unreliable. In a discussion after a presentation of residual analysis in a seminar at Berkeley in 1993, William Cleveland, one of the fathers of residual analysis, admitted that it could not uncover lack of fit in more than four to five dimensions. The papers I have read on using residual analysis to check lack of fit are confined to data sets with two or three variables.

此外，如果在数据的基础上对模型进行修补，即删除变量或添加变量的非线性组合，则拟合优度检验不适用。残差分析同样不可靠。1993年，在伯克利的一个研讨会上，残差分析的创始人之一威廉·克利夫兰(William Cleveland)在发表残差分析之后的讨论中承认，残差分析无法发现超过4到5个维度的不匹配。我读过的那些使用残差分析来检查是否符合的论文都局限于有两个或三个变量的数据集。

With higher dimensions, the interactions between the variables can produce passable residual plots for a variety of models. A residual plot is a goodness-offit test, and lacks power in more than a few dimensions. An acceptable residual plot does not imply that the model is a good fit to the data.

在高维情况下，变量之间的相互作用可以产生可通行的残差图对于各种各样的模型。残差图是一种拟合优度检验方法，在很多方面都没有作用。可接受的残差图并不意味着模型与数据很好地拟合。

There are a variety of ways of analyzing residuals. For instance, Landwher, Preibon and Shoemaker (1984, with discussion) gives a detailed analysis of fitting a logistic model to a three-variable data set using various residual plots. But each of the four discussants present other methods for the analysis. One is left with an unsettled sense about the arbitrariness of residual analysis.

有很多种分析残差的方法。例如，Landwher, Preibon 和 Shoemaker(1984，带讨论)详细分析了使用各种残差图拟合三变量数据集的逻辑模型。但是 四位讨论者都提出了其他的分析方法。人们对残差分析的任意性有一种不确定的感觉。

Misleading conclusions may follow from data models that pass goodness-of-fit tests and residual checks. But published applications to data often show little care in checking model fit using these methods or any other. For instance, many of the current application articles in JASA that fit data models have very little discussion of how well their model fits the data. The question of how well the model fits the data is of secondary importance compared to the construction of an ingenious stochastic model.

通过拟合性检验和残留检验的数据模型可能会得出误导的结论。但是对数据发布的应用程序通常很少使用这些方法或其他方法来检查模型是否合适。例如，当前许多适合数据模型的JASA应用程序文章很少讨论它们的模型适合数据的程度。与构建巧妙的随机模型相比，模型与数据吻合程度的问题是次要的。

###  5.3 The Multiplicity of Data Models
### 5.3 数据模型的多样性

One goal of statistics is to extract information from the data about the underlying mechanism producing the data. The greatest plus of data modeling is that it produces a simple and understandable picture of the relationship between the input variables and responses. For instance, logistic regression in classification is frequently used because it produces a linear combination of the variables with weights that give an indication of the variable importance. The end result is a simple picture of how the prediction variables affect the response variable plus confidence intervals for the weights. Suppose two statisticians, each one with a different approach to data modeling, fit a model to the same data set. Assume also that each one applies standard goodness-of-fit tests, looks at residuals, etc., and is convinced that their model fits the data. Yet the two models give different pictures of nature's mechanism and lead to different conclusions.

统计的目标之一是从数据中提取关于产生数据的底层机制的信息。数据建模的最大优点是，它生成了输入变量和响应变量之间关系的简单且易于理解的图像。 例如，在分类中经常使用逻辑回归，因为它产生变量的线性组合与权重给出变量的重要性指示。最终结果是预测变量如何影响响应变量以及权重的置信区间的简单图像。假设有两个统计学家，他们各自采用不同的数据建模方法，将一个模型应用于相同的数据集。还假设每个人都采用标准的拟合优度检验，查看残差等，并确信他们的模型符合数据。然而，这两个模型对自然界的机制给出了不同的描述，并得出了不同的结论。

McCullah and Nelder (1989) write "Data will often point with almost equal emphasis on several possible models, and it is important that the statistician recognize and accept this." Well said, but different models, all of them equally good, may give different pictures of the relation between the predictor and response variables. The question of which one most accurately reflects the data is difficult to resolve. One reason for this multiplicity is that goodness-of-fit tests and other methods for checking fit give a yes-no answer. With the lack of power of these tests with data having more than a small number of dimensions, there will be a large number of models whose fit is acceptable. There is no way, among the yes-no methods for gauging fit, of determining which is the better model. A few statisticians know this. Mountain and Hsiao (1989) write, "It is difficult to formulate a comprehensive model capable of encompassing all rival models. Furthermore, with the use of finite samples, there are dubious implications with regard to the validity and power of various encompassing tests that rely on asymptotic theory."

McCullah 和 Nelder(1989)写道:“数据通常会以几乎相同的重点指向几个可能的模型，统计学家认识和接受这一点很重要。”说得好，但不同的模型，它们都一样好，可能会给出预测器和反应变量之间关系的不同图景。哪一个最准确地反映了数据的问题是很难解决的。这种多样性的一个原因是适合度测试和其他检查适合度的方法给出的答案是“是”或“不是”。由于缺乏这些测试的威力与数据有多个维度， 将有大量的模型，其适合是可接受的。在衡量是否合适的方法中，没有办法确定哪个模型更好。一些统计学家知道这一点。山和萧(1989)写道:“很难建立一个能够囊括所有竞争对手的综合模型。此外，在使用有限样本的情况下，依赖于渐近理论的各种包围测试的有效性和能力存在可疑的含义。”

Data models in current use may have more damaging results than the publications in the social sciences based on a linear regression analysis. Just as the $5 \%$ level of significance became a de facto standard for publication, the Cox model for the analysis of survival times and logistic regression for survivenonsurvive data have become the de facto standard for publication in medical journals. That different survival models, equally well fitting, could give different conclusions is not an issue.

目前使用的数据模型可能比基于线性回归分析的社会科学出版物具有更大的破坏性结果。正如5%显著性水平成为了发表的事实上的标准，生存时间分析的Cox模型和生存-非生存数据的逻辑回归已经成为医学期刊发表的事实上的标准。不同的生存 模式，同样适合，可以得出不同的结论，这不是问题。

###  5.4 Predictive Accuracy
### 5.4 预测精度

The most obvious way to see how well the model box emulates nature's box is this: put a case $\mathbf{x}$ down nature's box getting an output $y$. Similarly, put the same case $\mathbf{x}$ down the model box getting an output $y^{\prime}$. The closeness of $y$ and $y^{\prime}$ is a measure of how good the emulation is. For a data model, this translates as: fit the parameters in your model by using the data, then, using the model, predict the data and see how good the prediction is.

要想知道模型盒与自然盒的相似程度，最明显的方法是这样的:在自然盒上放一个情况x，得到输出y。同样，在模型盒上放同样的情况x，得到输出$y^{\prime}$。y的亲密度$y^{\prime}$ 是衡量仿真有多好。对于数据模型，这可以理解为:通过使用数据拟合模型中的参数，然后使用模型预测数据，看看预测有多好。

Prediction is rarely perfect. There are usually many unmeasured variables whose effect is referred to as "noise." But the extent to which the model box emulates nature's box is a measure of how well our model can reproduce the natural phenomenon producing the data.

预测很少是完美的。通常有许多无法测量的变量，它们的影响被称为“噪声”。但是模型盒模仿自然盒子的程度是衡量我们的模型复制产生数据的自然现象的好坏的标准。

McCullagh and Nelder (1989) in their book on generalized linear models also think the answer is obvious. They write, "At first sight it might seem as though a good model is one that fits the data very well; that is, one that makes $\hat{\mu}$ (the model predicted value) very close to $y$ (the response value).” Then they go on to note that the extent of the agreement is biased by the number of parameters used in the model and so is not a satisfactory measure. They are, of course, right. If the model has too many parameters, then it may overfit the data and give a biased estimate of accuracy. But there are ways to remove the bias. To get a more unbiased estimate of predictive accuracy, cross-validation can be used, as advocated in an important early work by Stone (1974). If the data set is larger, put aside a test set Mosteller and Tukey (1977) were early advocates of cross-validation. They write, "Cross-validation is a natural route to the indication of the quality of any data-derived quantity.... We plan to cross-validate carefully wherever we can."

McCullagh 和 Nelder(1989)在他们关于广义线性模型的书中也认为答案是显而易见的。他们写道:“乍一看，一个好的模型似乎是一个非常适合数据的模型;即使模型预测值 µˆ与响应值y非常接近。然后他们继续指出，协议的程度受到模型中使用的参数数量的影响，因此不是一个令人满意的测量。当然，他们是对的。如果模型有太多的参数，那么它可能会过度拟合数据，并给出一个有偏差的精度估计。但还是有办法消除偏见的。如Stone(1974)在其重要的早期工作中所提倡的那样，为了获得更无偏的预测准确性估计值，可以使用交叉验证。如果数据集较大，则将测试集放在一边。Mosteller 和 Tukey(1977)是交叉验证的早期倡导者。他们写道:“交叉验证是指示任何数据衍生量的质量的自然途径...我们计划尽可能仔细地进行交叉验证。”

Judging by the infrequency of estimates of predictive accuracy in JASA, this measure of model fit that seems natural to me (and to Mosteller and Tukey) is not natural to others. More publication of predictive accuracy estimates would establish standards for comparison of models, a practice that is common in machine learning.

从 JASA 中预测准确性估计的频率来看，这种对我(以及 Mosteller 和 Tukey)来说似乎很自然的模型拟合方法对其他人来说就不自然了。更多的预测精度估计将建立模型比较的标准，这是机器学习中常见的做法。

## 6 THE LIMITATIONS OF DATA MODELS
## 6. 数据模型的局限性

With the insistence on data models, multivariate analysis tools in statistics are frozen at discriminant analysis and logistic regression in classification and multiple linear regression in regression. Nobody really believes that multivariate data is multivariate normal, but that data model occupies a large number of pages in every graduate textbook on multivariate statistical analysis.

由于对数据模型的坚持，统计中的多元分析工具在判别分析中被冻结，在分类中被逻辑回归中冻结，在回归中被多元线性回归中冻结。没有人真正相信多元数据是多元正态的，但是在每一本关于多元统计分析的研究生教科书中，数据模型占据了大量的页面。

With data gathered from uncontrolled observations on complex systems involving unknown physical, chemical, or biological mechanisms, the a priori assumption that nature would generate the data through a parametric model selected by the statistician can result in questionable conclusions that cannot be substantiated by appeal to goodness-of-fit tests and residual analysis. Usually, simple parametric models imposed on data generated by complex systems, for example, medical data, financial data, result in a loss of accuracy and information as compared to algorithmic models (see Section 11).

通过对涉及未知物理、化学或生物机制的复杂系统的不受控制的观察收集的数据， 自然会通过统计学家选择的参数模型产生数据这一先验假设可能导致有问题的结论，而这些结论无法通过拟合优度检验和残差分析加以证实。通常，简单参数化模型强加于由复杂系统生成的数据，例如医疗数据、财务数据，与算法模型相比，会导致准确性和信息的损失(见第 11 节)。

There is an old saying "If all a man has is a hammer, then every problem looks like a nail." The trouble for statisticians is that recently some of the problems have stopped looking like nails. I conjecture that the result of hitting this wall is that more complicated data models are appearing in current published applications. Bayesian methods combined with Markov Chain Monte Carlo are cropping up all over. This may signify that as data becomes more complex, the data models become more cumbersome and are losing the advantage of presenting a simple and clear picture of nature's mechanism.

有句老话说:“如果一个人只有一把锤子， 那么每个问题看起来都像钉子。" 统计学家最近面临的有些问题看起来不再像钉子了。我猜想，碰到这堵墙的结果是，在当前发布的应用程序中出现了更复杂的数据模型。贝叶斯方法与马尔可夫链蒙特卡罗相结合的方法在世界各地都有出现。这可能意味着，随着数据变得更加复杂，数据模型变得更加繁琐，并失去了简单而清晰地描述自然机制的优势。

Approaching problems by looking for a data model imposes an a priori straight jacket that restricts the ability of statisticians to deal with a wide range of statistical problems. The best available solution to a data problem might be a data model; then again it might be an algorithmic model. The data and the problem guide the solution. To solve a wider range of data problems, a larger set of tools is needed. Perhaps the damaging consequence of the insistence on data models is that statisticians have ruled themselves out of some of the most interesting and challenging statistical problems that have arisen out of the rapidly increasing ability of computers to store and manipulate data. These problems are increasingly present in many fields, both scientific and commercial, and solutions are being found by nonstatisticians.

通过寻找数据模型来处理问题强加了一个先验的直套，这限制了统计学家处理广泛统计问题的能力。数据问题的最佳可用解决方案可能是数据模型;这也可能是一个算法模型。数据和问题指导解决方案。要解决更广泛的数据问题，就需要一套更大的工具。也许，坚持使用数据模型的破坏性后果是，由于计算机存储和操作数据的能力迅速增强，一些最有趣、最具挑战性的统计问题出现了，而统计学家们却把自己排除在了这些问题之外。这些问题越来越多地出现在许多领域，无论是科学领域还是商业领域，非统计学家正在寻找解决方案。

## 7 ALGORITHMIC MODELING
## 7 算法建模

Under other names, algorithmic modeling has been used by industrial statisticians for decades. See, for instance, the delightful book "Fitting Equations to Data" (Daniel and Wood, 1971). It has been used by psychometricians and social scientists. Reading a preprint of Gifi's book (1990) many years ago uncovered a kindred spirit. It has made small inroads into the analysis of medical data starting with Richard Olshen's work in the early 1980 s. For further work, see Zhang and Singer (1999). Jerome Friedman and Grace Wahba have done pioneering work on the development of algorithmic methods. But the list of statisticians in the algorithmic modeling business is short, and applications to data are seldom seen in the journals. The development of algorithmic methods was taken up by a community outside statistics.

在其他名字下，算法建模已经被工业统计学家使用了几十年。例如，你可以看看令人愉快的《用数据拟合方程》(Daniel and Wood, 1971)。它已经被心理测量学家和社会科学家使用。多年前，读了吉吉这本书(1990 年)的预印本后，我发现了一种相似的精神。从 Richard Olshen 在 20 世纪 80 年代早期的工作开始，它已经在医学数据分析方面取得了小小的进展。关于进一步的工作，请参阅 Zhang 和 Singer(1999) 。Jerome Friedman和Grace Wahba在算法方法的开发上做了开创性的工作。但是，从事算法建模业务的统计学家很少，而且在期刊上很少看到对数据的应用。算法方法的开发是由统计学之外的一个社区负责的。

###  7.1 A New Research Community
### 7.1 一个新的研究社区

In the mid- 1980 s two powerful new algorithms for fitting data became available: neural nets and decision trees. A new research community using these tools sprang up. Their goal was predictive accuracy. The community consisted of young computer scientists, physicists and engineers plus a few aging statisticians. They began using the new tools in working on complex prediction problems where it was obvious that data models were not applicable: speech recognition, image recognition, nonlinear time series prediction, handwriting recognition, prediction in financial markets.

在20世纪80年代中期，两种强大的数据拟合算法出现了:神经网络和决策树。一个新的使用这些工具的研究团体出现了。他们的目标是预测的准确性。这个社区由年轻的计算机科学家、物理学家和工程师以及一些上了年纪的统计学家组成。他们开始使用新工具来解决复杂的预测问题，这些问题显然是数据模型不适用的:语音识别，图像识别，非线性时间序列预测，手写识别，金融市场预测。

Their interests range over many fields that were once considered happy hunting grounds for statisticians and have turned out thousands of interesting research papers related to applications and methodology. A large majority of the papers analyze real data. The criterion for any model is what is the predictive accuracy. An idea of the range of research of this group can be got by looking at the Proceedings of the Neural Information Processing Systems Conference (their main yearly meeting) or at the Machine Learning Journal.

他们的兴趣涵盖了许多领域，这些领域曾经被认为是统计学家的快乐狩猎场，并已经发表了数千篇与应用和方法论相关的有趣的研究论文。大部分论文分析的是真实数据。任何模型的标准都是预测的准确性。这个小组的研究范围可以从《神经信息处理系统会议纪要》(他们的主要年度会议)或《机器学习杂志》上了解。

###  7.2 Theory in Algorithmic Modeling
### 7.2 算法建模理论

Data models are rarely used in this community. The approach is that nature produces data in a black box whose insides are complex, mysterious, and, at least, partly unknowable. What is observed is a set of $\mathbf{x}$ 's that go in and a subsequent set of $\mathbf{y}$ 's that come out. The problem is to find an algorithm $f(\mathbf{x})$ such that for future $\mathbf{x}$ in a test set, $f(\mathbf{x})$ will be a good predictor of $\mathbf{y}$.

在这个社区中很少使用数据模型。其方法是，自然的在一个内部复杂、神秘、至少部分不可知的黑匣子中生成数据。我们观察到的是一组输入的x和另一组输出的 y。问题是找到一个算法 $f(\mathbf{x})$ 对于测试集中未来的 x,$f(\mathbf{x})$可以很好地预测 y。

The theory in this field shifts focus from data models to the properties of algorithms. It characterizes their "strength" as predictors, convergence if they are iterative, and what gives them good predictive accuracy. The one assumption made in the theory is that the data is drawn i.i.d. from an unknown multivariate distribution.

该领域的理论重点从数据模型转移到算法的属性。它将它们的“强度”描述为预测器，迭代时的收敛性，以及使它们具有良好预测准确性的因素。这个理论的一个假设是这些数据来自一个未知的多元分布。

There is isolated work in statistics where the focus is on the theory of the algorithms. Grace Wahba's research on smoothing spline algorithms and their applications to data (using crossvalidation) is built on theory involving reproducing kernels in Hilbert Space (1990). The final chapter of the CART book (Breiman et al., 1984) contains a proof of the asymptotic convergence of the CART algorithm to the Bayes risk by letting the trees grow as the sample size increases. There are others, but the relative frequency is small.

在统计学中，专注于算法理论的工作是孤立的。Grace Wahba 对平滑样条算法及其在数据中的应用 (使用交叉验证)的研究是建立在希尔伯特空间(1990) 的核再生理论基础上的。CART 书(Breiman et al.， 1984)的最后一章证明了 CART 算法对贝叶斯风险的渐近收敛性，方法是让树随着样本量的增加而增长。还有其他的，但相对频率较小。

Theory resulted in a major advance in machine learning. Vladimir Vapnik constructed informative bounds on the generalization error (infinite test set error) of classification algorithms which depend on the "capacity" of the algorithm. These theoretical bounds led to support vector machines (see Vapnik, 1995,1998 ) which have proved to be more accurate predictors in classification and regression then neural nets, and are the subject of heated current research (see Section 10).

理论使机器学习取得了重大进展。Vladimir Vapnik 构造了分类算法泛化误差(无限测试集误差)的信息界，这取决于算法的“容量”。这些理论界限导致支持向量机(见 Vapnik, 1995, 1998)，它被证明是分类和回归中比神经网络更准确的预测器，也是当前研究的热点(见第 10 节)。

My last paper "Some infinity theory for tree ensembles" (Breiman, 2000) uses a function space analysis to try and understand the workings of tree ensemble methods. One section has the heading, "My kingdom for some good theory." There is an effective method for forming ensembles known as "boosting," but there isn't any finite sample size theory that tells us why it works so well.

我的上一篇论文“树集合的无限理论”(Breiman, 2000)使用函数空间分析来尝试和理解树集合方法 的工作原理。其中一节的标题是:“我的领域为一些好的理论。”有一种形成整体的有效方法被称为 “助推”，但没有任何有限样本容量理论告诉我们为什么它工作得如此好。

###  7.3 Recent Lessons

### 7.3 最近的经验

The advances in methodology and increases in predictive accuracy since the mid-1980s that have occurred in the research of machine learning has been phenomenal. There have been particularly exciting developments in the last five years. What has been learned? The three lessons that seem most important to one:

自20世纪80年代中期以来，机器学习的研究在方法论和预测准确性方面取得了显著进展。在过去五年中，有了特别令人兴奋的发展。我们学到了什么?最重要的三个教训重要的一个:

Rashomon: the multiplicity of good models;

罗生门:优秀模型的多样性;

Occam: the conflict between simplicity and accuracy;

奥卡姆:简单与准确之间的冲突;

Bellman: dimensionality_curse or blessing.

服务生:维度——诅咒还是祝福。

## 8. RASHOMON AND THE MULTIPLICITY OF GOOD MODELS}
## 8. 罗生门和众多的好模型

Rashomon is a wonderful Japanese movie in which four people, from different vantage points, witness an incident in which one person dies and another is supposedly raped. When they come to testify in court, they all report the same facts, but their stories of what happened are very different.

《罗生门》是一部精彩的日本电影，四个人从不同的角度，目睹了一场事件，其中一人死亡，另一人据称被强奸。当他们在法庭上作证时。他们都报告了同样的事实，但他们对所发生事情的描述却非常不同。

What I call the Rashomon Effect is that there is often a multitude of different descriptions [equations $f(\mathbf{x})]$ in a class of functions giving about the same minimum error rate. The most easily understood example is subset selection in linear regression. Suppose there are 30 variables and we want to find the best five variable linear regressions. There are about 140,000 five-variable subsets in competition. Usually we pick the one with the lowest residual sum-of-squares (RSS), or, if there is a test set, the lowest test error. But there may be (and generally are) many five-variable equations that have RSS within $1.0 \%$ of the lowest RSS (see Breiman, 1996a). The same is true if test set error is being measured.

我称之为罗生门效应的是，通常有许多不同的描述[equations $f(\mathbf{x})]$ 在一类函数中给出相同的最小错误率。最容易理解的例子是线性回归中的子集选择。假设有30个变量，我们想找出最佳的5个变量线性回归。大约有140,000个五变量子集在竞争。通常我们会选择残差平方和(RSS)最小的那个，或者，如果有一个测试集，测试误差最小的那个。但是，可能有(而且通常是)许多五变量方程的 RSS 值在最低 RSS 值的 1.0%以内(见 Breiman, 1996a)。如果测试集的误差正在被测量，同样是正确的。

So here are three possible pictures with RSS or test set error within $1.0 \%$ of each other:

以下是三张可能的 RSS 或测试设置错误在 1.0% 以内的图片:

Picture 1

图片 1
$$
\begin{aligned}
y=& 2.1+3.8 x_{3}-0.6 x_{8}+83.2 x_{12} \\
&-2.1 x_{17}+3.2 x_{27}
\end{aligned}
$$

Picture 2

图片 2

$$
\begin{aligned}
y=&-8.9+4.6 x_{5}+0.01 x_{6}+12.0 x_{15} \\
&+17.5 x_{21}+0.2 x_{22},
\end{aligned}
$$

Picture 3

图片3

$$
\begin{aligned}
y=&-76.7+9.3 x_{2}+22.0 x_{7}-13.2 x_{8} \\
&+3.4 x_{11}+7.2 x_{28} .
\end{aligned}
$$

Which one is better? The problem is that each one tells a different story about which variables are important.

哪一个更好? 问题是每一个变量都讲述了不同的故事，哪些变量是重要的。

The Rashomon Effect also occurs with decision trees and neural nets. In my experiments with trees, if the training set is perturbed only slightly, say by removing a random 2-3\% of the data, I can get a tree quite different from the original but with almost the same test set error. I once ran a small neural net 100 times on simple three-dimensional data reselecting the initial weights to be small and random on each run. I found 32 distinct minima, each of which gave a different picture, and having about equal test set error.

罗生门效应也发生在决策树和神经网络上。在我的决策树实验中，如果训练集只受到轻微的干扰，比如通过删除2-3%的随机数据，我可以得到一棵与原始树完全不同的树，但测试集误差几乎相同。我曾经经营过一家小型的神经网络对简单的三维数据进行100次重选初始权值，每次运行时都是随机的。我找到了32个不同的极小值，每个极小值都给出了不同的图像，它们的测试集误差大致相同。

This effect is closely connected to what I call instability (Breiman, $1996 \mathrm{a}$ ) that occurs when there are many different models crowded together that have about the same training or test set error. Then a slight perturbation of the data or in the model construction will cause a skip from one model to another. The two models are close to each other in terms of error, but can be distant in terms of the form of the model.

这种效应与我所说的不稳定性密切相关(Breiman, 1996a)，当许多不同的模型聚集在一起时，就会产生相同的训练或测试集错误。然后，数据或模型构造中的微小扰动将导致从一个模型跳到另一个模型。这两个模型在误差方面是接近的，但在模型的形式方面可能是遥远的。

If, in logistic regression or the Cox model, the common practice of deleting the less important covariates is carried out, then the model becomes unstable-there are too many competing models. Say you are deleting from 15 variables to 4 variables. Perturb the data slightly and you will very possibly get a different four-variable model and a different conclusion about which variables are important. To improve accuracy by weeding out less important covariates you run into the multiplicity problem. The picture of which covariates are important can vary significantly between two models having about the same deviance.

如果，在逻辑回归或Cox模型中，通常的做法是删除不太重要的协变量，那么模型就变得不稳定-有太多的竞争模型。假设你要从 15 个变量中删除 4 个变量。稍微扰动一下数据，你很可能会得到一个不同的四变量模型，以及关于哪些变量重要的不同结论。为了通过剔除不太重要的协变量来提高准确性，就会遇到多重性问题。在两个有着相同偏差的模型之间，关于哪个协变量是重要的图像会有很大的不同。

Aggregating over a large set of competing models can reduce the nonuniqueness while improving accuracy. Arena et al. (2000) bagged (see Glossary) logistic regression models on a data base of toxic and nontoxic chemicals where the number of covariates in each model was reduced from 15 to 4 by standard best subset selection. On a test set, the bagged model was significantly more accurate than the single model with four covariates. It is also more stable. This is one possible fix. The multiplicity problem and its effect on conclusions drawn from models needs serious attention.

通过聚集大量的竞争模型可以减少非唯一性，同时提高准确性。Arena等人(2000)将有毒和无毒化学品数据库中的逻辑回归模型bagged(见术语表)【译者注：专业名词】，通过标准最佳子集选择，每个模型中的协变量数量从15个减少到4个。在一个测试集上,bagged模型【译者注：专业名词】明显比单模型的四个协变量更准确。它也更稳定。这是一种可能的解决方法。多重性问题及其对模型结论的影响需要认真关注。

## 9. OCCAM AND SIMPLICITY VS. ACCURACY}
## 9. 奥卡姆， 简单 vs. 准确

Occam's Razor, long admired, is usually interpreted to mean that simpler is better. Unfortunately, in prediction, accuracy and simplicity (interpretability) are in conflict. For instance, linear regression gives a fairly interpretable picture of the $\mathbf{y}, \mathbf{x}$ relation. But its accuracy is usually less than that of the less interpretable neural nets. An example closer to my work involves trees.

奥卡姆剃刀，长期以来备受推崇，通常被解释为越简单越好。不幸的是,在预测中，准确性和简洁性(可解释性)是矛盾的。例如，线性回归给出了一个相当可解释的 xy 关系图。但它的准确性通常低于那些难以解释的神经网络。与我的工作有关的一个例子是树。

On interpretability, trees rate an $\mathrm{A}+$ A project I worked on in the late 1970 s was the analysis of delay in criminal cases in state court systems. The Constitution gives the accused the right to a speedy trial. The Center for the State Courts was concerned TABLE 1

在可解释性方面，树木的评级为 A+。我在 20 世 纪 70 年代末做的一个项目是分析州法院系统中刑事案件的拖延。宪法赋予被告迅速审判的权利。州法院中心对此表示关切

【表格不对】
\begin{tabular}{ccc} 
Data set descriptions \\
\hline Test Sample size & Variables & Classes \\
\hline$-$ & 9 & 2 \\
$-$ & 34 & 2 \\
$-$ & 8 & 2 \\
$-$ & 9 & 6 \\
$-$ & 35 & 19 \\
5000 & 16 & 26 \\
2000 & 36 & 6 \\
14,500 & 9 & 7 \\
1,186 & 60 & 3 \\
2,007 & 256 & 10 \\
\hline
\end{tabular}

that in many states, the trials were anything but speedy. It funded a study of the causes of the delay. I visited many states and decided to do the analysis in Colorado, which had an excellent computerized court data system. A wealth of information was extracted and processed.

在许多州，审判一点也不迅速。它资助了一项关于延误原因的研究。我访问了许多州，并决定在科罗拉多州进行分析，那里有一个杰出的电脑化法庭数据系统。大量的信息被提取和处理。

The dependent variable for each criminal case was the time from arraignment to the time of sentencing. All of the other information in the trial history were the predictor variables. A large decision tree was grown, and I showed it on an overhead and explained it to the assembled Colorado judges. One of the splits was on District $\mathrm{N}$ which had a larger delay time than the other districts. I refrained from commenting on this. But as I walked out I heard one judge say to another, "I knew those guys in District N were dragging their feet."

每个刑事案件的因变量是从提审到宣判的时间。试验史上的所有其他信息都是预测变量。我在头顶上展示了一棵巨大的决策树，并向聚集在一起的科罗拉多法官们解释了它。其中一个分裂是在 N 区，它的延迟时间比其他区大。我没有对此发表评论。但当我走出去的时候，我听到一个法官对另一个说，“我知道 N 区的那些家伙在拖延时间。”

While trees rate an A+ on interpretability, they are good, but not great, predictors. Give them, say, a B on prediction.

虽然树木在可解释性方面的评分是 A+，但它们是很好的预测者，但不是伟大的。比如说，给他们的预测打个 B。

### 9.1 Growing Forests for Prediction
### 9.1 用于预测的生长期森林

Instead of a single tree predictor, grow a forest of trees on the same data-say 50 or 100 . If we are classifying, put the new $\mathbf{x}$ down each tree in the forest and get a vote for the predicted class. Let the forest prediction be the class that gets the most votes. There has been a lot of work in the last five years on wavs to grow the forest. All of the well-known methods grow the forest by perturbing the training set, growing a tree on the perturbed training set, perturbing the training set again, growing another tree, etc. Some familiar methods are bagging (Breiman, 1996b), boosting (Freund and Schapire, 1996), arcing (Breiman, 1998), and additive logistic regression (Friedman, Hastie and Tibshirani, 1998).

而不是一个单一的树预测器，种植一个森林的树木在相同的数据，比如 50 或 100。如果我们进行分类，在森林中的每一棵树上放上新的 x，并为预测的类投票。让森林预测成为得票最多的类。在过去的五年里，有很多关于如何种植森林的工作。所有知名的方法都是通过扰动训练集来生长森林，在扰动训练集上生长一棵树，再次扰动训练集,生长另一棵树等等。一些熟悉的方法有套袋【译者注：专业名词】(Breiman, 1996b)、助推(Freund and Schapire, 1996)、arccing (Breiman, 1998)和加性逻辑回归(Friedman, Hastie and Tibshirani, 1998)。

My preferred method to date is random forests. In this approach successive decision trees are grown by introducing a random element into their construction. For example, suppose there are 20 predictor variables. At each node choose several of the 20 at random to use to split the node. Or use a random combination of a random selection of a few variables. This idea appears in Ho (1998), in Amit and Geman (1997) and is developed in Breiman (1999).

到目前为止，我最喜欢的方法是随机森林。在这种方法中，通过在决策树的构造中引入随机元素来生长连续的决策树。例如，假设有20个预测因子变量。在每个节点上，从20个节点中随机选择几个来分割节点。或者使用随机选择的几个变量的随机组合。这一观点出现在 Ho(1998),Amit和german(1997),并在 Breiman(1999)得到发展。

### 9.2 Forests Compared to Trees
### 9.2 森林与树木的比较

We compare the performance of single trees (CART) to random forests on a number of small and large data sets, mostly from the UCI repository (ftp.ics.uci.edu/pub/MachineLearningDatabases). A summary of the data sets is given in Table 1 .

我们比较了单个树(CART)和随机森林在许多大小数据集上的性能，这些数据集主要来自 UCI 存储库(ftp.ics.uci.edu/pub MachineLearningDatabases)。 表 1 给出了数据集的总结。

Table 2 compares the test set error of a single tree to that of the forest. For the five smaller data sets above the line, the test set error was estimated by leaving out a random $10 \%$ of the data, then running CART and the forest on the other $90 \%$. The left-out $10 \%$ was run down the tree and the forest and the error on this $10 \%$ computed for both. This was repeated 100 times and the errors averaged. The larger data sets below the line came with a separate test set. People who have been in the classification field for a while find these increases in accuracy startling. Some errors are halved. Others are reduced by one-third. In regression, where the

表 2 比较了单个树与森林的测试集误差。对于这条线以上的 5 个较小的数据集，测试集的误差估计是通过遗漏10%的随机数据，然后运行CART和森林对其余 90%。遗漏的10%在树和森林中运行，这10%的误差为两者计算。这一过程重复了100 次，误差平均下来。线下较大的数据集带有一个单独的测试集。在分类领域工作了一段时间的人会发现准确率的提高是惊人的。有些错误被减半。其他的减少了三分之一。在回归中

【表格不对】
\begin{tabular}{lrr} 
Test set misclassification error (\%) \\
\hline Data set & Forest & Single tree \\
\hline Breast cancer & $2.9$ & $5.9$ \\
Ionosphere & $5.5$ & $11.2$ \\
Diabetes & $24.2$ & $25.3$ \\
Glass & $22.0$ & $30.4$ \\
Soybean & $5.7$ & $8.6$ \\
Letters & $3.4$ & $12.4$ \\
Satellite & $8.6$ & $14.8$ \\
Shuttle $\times 10^{3}$ & $7.0$ & $62.0$ \\
DNA & $3.9$ & $6.2$ \\
Digit & $6.2$ & $17.1$ \\
\hline
\end{tabular}

TABLE 2 forest prediction is the average over the individual tree predictions, the decreases in mean-squared test set error are similar.

TABLE 2 森林预测是对单个树的平均预测，其减少的均方检验集误差是相似的。

### 9.3 Random Forests are A + Predictors
### 9.3 随机森林是 A + 预测因子

The Statlog Project (Mitchie, Spiegelhalter and Taylor, 1994) compared 18 different classifiers. Included were neural nets, CART, linear and quadratic discriminant analysis, nearest neighbor, etc. The first four data sets below the line in Table 1 were the only ones used in the Statlog Project that came with separate test sets. In terms of rank of accuracy on these four data sets, the forest comes in $1,1,1,1$ for an average rank of $1.0$. The next best classifier had an average rank of $7.3$.

Statlog 项 目 (Mitchie, Spiegelhalter 和 Taylor, 1994)比较了 18 个不同的分类器。包括神经网络、CART、线性和二次判别分析、最近邻【译者注：专业名词】等。 表 1 下面的前四个数据集是 Statlog 项目中唯一使用的 带有独立测试集的数据集。就这四个数据集的精度等级而言，森林的平均等级为 1.0，则为 1,1,1,1。 其次最好的分类器的平均排名为 7.3。

The fifth data set below the line consists of $16 \times 16$ pixel gray scale depictions of handwritten ZIP Code numerals. It has been extensively used by AT\&T Bell Labs to test a variety of prediction methods. A neural net handcrafted to the data got a test set error of $5.1 \%$ vs. $6.2 \%$ for a standard run of random forest.

一行以下的第五个数据集由 16×16 像素灰度描述的手写邮政编码数字组成。它已经被 AT&T 贝尔实验室广泛地用于测试各种预测方法。对数据手工制作的神经网络得到的测试集误差为5.1%，而随机森林标准运行的测试集误差为 6.2%。

### 9.4 The Occam Dilemma
### 9.4 奥卡姆困境

So forests are A+ predictors. But their mechanism for producing a prediction is difficult to understand. Trying to delve into the tangled web that generated a plurality vote from 100 trees is a Herculean task. So on interpretability, they rate an $\mathrm{F}$. Which brings us to the Occam dilemma:

所以森林是 A+预测者。但它们产生预测的机制很难理解。试图深入探究这张从 100棵树中获得多数选票的错综复杂的网络是一项艰巨的任务。所以在可解释性方面，他们给 f 打分，这把我们带到了奥卡姆困境:

- Accuracy generally requires more complex prediction methods. Simple and interpretable functions do not make the most accurate predictors.

•准确性通常需要更复杂的预测方法。简单和可解释的函数并不是最准确的预测器。

Using complex predictors may be unpleasant, but the soundest path is to go for predictive accuracy first, then try to understand why. In fact, Section 10 points out that from a goal-oriented statistical viewpoint, there is no Occam's dilemma. (For more on Occam's Razor see Domingos, 1998, 1999.)

使用复杂的预测器可能会让人不愉快，但最合理的方法是首先追求预测的准确性， 然后尝试理解原因。事实上，第 10 节指出，从目标导向的统计观点来看，不存在奥卡姆困境。(欲了解更多奥卡姆剃刀的信息，请参阅 Domingos, 1998, 1999)

## 10. BELLMAN AND THE CURSE OF DIMINSIONALITY
## 10. 贝尔曼和维度诅咒

The title of this section refers to Richard Bellman's famous phrase, "the curse of dimensionality." For decades, the first step in prediction methodology was to avoid the curse. If there were too many prediction variables, the recipe was to find a few features (functions of the predictor variables) that "contain most of the information" and then use these features to replace the original variables. In procedures common in statistics such as regression, logistic regression and survival models the advised practice is to use variable deletion to reduce the dimensionality. The published advice was that high dimensionality is dangerous. For instance, a well-regarded book on pattern recognition (Meisel, 1972) states "the features... must be relatively few in number." But recent work has shown that dimensionality can be a blessing.

本节的标题引用了理查德·贝尔曼的名言“维 度的诅咒”。 几十年来， 预测方法的第一步就是 避免诅咒。 如果有太多的预测变量， 诀窍就是找 到一些“包含大部分信息”的特性(预测变量的函 数)，然后使用这些特性来替代原始变量。在统计 学中常见的程序， 如回归、逻辑回归和生存模型， 建议的做法是使用变量删除来减少维度。 发表的建议是， 高维度是危险的。 例如， 一本颇受好评的关于模式识别的书(Meisel, 1972)写 道:“特征?？？数量一定相对较少。但最近的研究 表明，维度可能是一种祝福。

### 10.1 Digging It Out in Small Pieces
### 10.1 小块挖掘

Reducing dimensionality reduces the amount of information available for prediction. The more predictor variables, the more information. There is also information in various combinations of the predictor variables. Let's try going in the opposite direction:

降维减少了可供预测的信息量。预测变量越多，信息就越多。在预测变量的不同组合中也有信息。让我们试着往相反的方向走:

- Instead of reducing dimensionality, increase it by adding many functions of the predictor variables.

•通过增加预测变量的许多函数来增加维数，而不是减少维数。

There may now be thousands of features. Each potentially contains a small amount of information. The problem is how to extract and put together these little pieces of information. There are two outstanding examples of work in this direction, The Shape Recognition Forest (Y. Amit and D. Geman, 1997) and Support Vector Machines (V. Vapnik, 1995, 1998).

现在可能有成千上万的功能。每一个都可能包含少量的信息。问题是如何提取和整合这些小信息片段。在这方面有两个杰出的例子，形状识别森林(Y. Amit 和 D. Geman, 1997)和支持向量机(V. Vapnik, 1995, 1998)。

### 10.2 The Shape Recognition Forest
### 10.2 形状识别森林

In 1992 , the National Institute of Standards and Technology (NIST) set up a competition for machine algorithms to read handwritten numerals. They put together a large set of pixel pictures of handwritten numbers $(223,000)$ written by over 2,000 individuals. The competition attracted wide interest, and diverse approaches were tried.

1992 年，美国国家标准与技术研究所(NIST)设立了一项机器算法阅读手写数字的竞赛。他们将2000多人手写的数字(223,000)的大量像素图片组合在一起。比赛吸引了广泛的兴趣，并尝试了不同的方法。

The Amit-Geman approach defined many thousands of small geometric features in a hierarchical assembly. Shallow trees are grown, such that at each node, 100 features are chosen at random from the appropriate level of the hierarchy; and the optimal split of the node based on the selected features is found.

Amit-Geman 【译者注：专业名词】方法在一个分层装配中定义了成千上万个小的几何特征。生长浅树，在每个节点上，从合适的层次中随机选择 100 个特征;并根据所选特征找到最优的节点分割。

When a pixel picture of a number is dropped down a single tree, the terminal node it lands in gives probability estimates $p_{0}, \ldots, p_{9}$ that it represents numbers $0,1, \ldots, 9$. Over 1,000 trees are grown, the probabilities averaged over this forest, and the predicted number is assigned to the largest averaged probability.

当一个数字的像素图像被放到一棵树上时，它所在的终端节点给出的概率估计为 $p_{0}, \ldots, p_{9}$ 表示数字 $0,1, \ldots, 9$.超过 1000 棵树被种植，概率平均在这片森林，预测的数字被分配到最大的平均概率。

Using a 100,000 example training set and a 50,000 test set, the Amit-Geman method gives a test set error of $0.7 \%$-close to the limits of human error.

使用 100,000 例训练集和 50,000 测试集，Amit-Geman【译者注：专业名词】 方法给出了0.7%的测试集误差，接近人为误差的极限。

### 10.3 Support Vector Machines
### 10.3 支持向量机

Suppose there is two-class data having prediction vectors in $M$-dimensional Euclidean space. The prediction vectors for class $\# 1$ are $\{\mathbf{x}(1)\}$ and those for class #2 are $\{\mathbf{x}(2)\}$. If these two sets of vectors can be separated by a hyperplane then there is an optimal separating hyperplane. "Optimal" is defined as meaning that the distance of the hyperplane to any prediction vector is maximal (see below).

假设 m 维欧氏空间中存在两类具有预测向量的数据。第 1 类的预测向量是 $\{\mathbf{x}(1)\}$ 和那些第 2 类是$\{\mathbf{x}(2)\}$。如果这两组向量可以被一个超平面分离,那么就存在一个最优分离超平面。“最优”的定义是指超平面到任何预测向量的距离是最大的(见下文)。

The set of vectors in $\{\mathbf{x}(1)\}$ and in $\{\mathbf{x}(2)\}$ that achieve the minimum distance to the optimal separating hyperplane are called the support vectors. Their coordinates determine the equation of the hyperplane. Vapnik (1995) showed that if a separating hyperplane exists, then the optimal separating hyperplane has low generalization error (see Glossary).

$\{\mathbf{x}(1)\}$和$\{\mathbf{x}(2)\}$达到最优分离超平面的最小距离的称为支持向量。它们的坐标决定了超平面的方程。Vapnik(1995)指出， 如果存在分离超平面，那么最优分离超平面具有较低的泛化误差(见术语表)。

![](https://cdn.mathpix.com/cropped/0ff65c4b1d28187c8f7d0addfa01c783-11.jpg?height=167&width=364&top_left_y=436&top_left_x=163)

In two-class data, separability by a hyperplane does not often occur. However, let us increase the dimensionality by adding as additional predictor variables all quadratic monomials in the original predictor variables; that is, all terms of the form $x_{m 1} x_{m 2}$. A hyperplane in the original variables plus quadratic monomials in the original variables is a more complex creature. The possibility of separation is greater. If no separation occurs, add cubic monomials as input features. If there are originally 30 predictor variables, then there are about 40,000 features if monomials up to the fourth degree are added.

在两类数据中，超平面的可分性并不经常发生。然而，让我们通过在原始预测变量中添加所有二次单项作为额外预测变量来增加维数;即$x_{m 1} x_{m 2}$形式的所有术语。原始变量中的超平面加上原始变量中的二次单项式是一个更复杂的东西。分离的可能性更大。如果没有分离发生，添加三次单项式作为输入特征。如果最初有 30 个预测变量，那么如果加入四阶以下的单项式，就会有大约40000个特征。

The higher the dimensionality of the set of features, the more likely it is that separation occurs. In the ZIP Code data set, separation occurs with fourth degree monomials added. The test set error is $4.1 \%$. Using a large subset of the NIST data base as a training set, separation also occurred after adding up to fourth degree monomials and gave a test set error rate of $1.1 \%$.

特征集的维数越高，分离发生的可能性就越大。在 ZIP Code 数据集中，添加了四度单项式就会发生分离。测试集错误为 4.1%。使用 NIST 数据库的一个大子集作为训练集，在加四次单项式后也会发生分离，给出了1.1%的测试集错误率。

Separation can always be had by raising the dimensionality high enough. But if the separating hyperplane becomes too complex, the generalization error becomes large. An elegant theorem (Vapnik, 1995) gives this bound for the expected generalization error:

分离总是可以通过提高足够高的维度来实现。但如果分离超平面过于复杂，泛化误差就会增大。一个优雅的定理(Vapnik, 1995)给出了预期泛化误差的界限:

$\operatorname{Ex}(\mathrm{GE}) \leq \mathrm{Ex}($ number of support vectors $) /(N-1)$

where $N$ is the sample size and the expectation is over all training sets of size $N$ drawn from the same underlying distribution as the original training set.

其中 N 是样本大小，期望是所有大小为 N 的训练集，这些训练集来自与原始训练集相同的底层分布。

The number of support vectors increases with the dimensionality of the feature space. If this number becomes too large, the separating hyperplane will not give low generalization error. If separation cannot be realized with a relatively small number of support vectors, there is another version of support vector machines that defines optimality by adding a penalty term for the vectors on the wrong side of the hyperplane.

支持向量的数量随着特征空间维数的增加而增加。如果这个数字太大时，分离超平面不能给出低的泛化误差。如果用相对较少的支持向量无法实现分离，那么还有另 一种支持向量机，它通过为超平面错误一侧的向量添加惩罚项来定义最优性。

Some ingenious algorithms make finding the optimal separating hyperplane computationally feasible. These devices reduce the search to a solution of a quadratic programming problem with linear inequality constraints that are of the order of the number $N$ of cases, independent of the dimension of the feature space. Methods tailored to this particular problem produce speed-ups of an order of magnitude over standard methods for solving quadratic programming problems.

一些巧妙的算法使寻找最优分离超平面在计算上可行。这些设备将搜索减少到一个二次规划问题的解与线性不等式约束的数量为 N 个情况，独立于特征空间的维数。针对这一特殊问题的方法比解决二次规划问题的标准方法产生一个数量级的加速。

Support vector machines can also be used to provide accurate predictions in other areas (e.g., regression). It is an exciting idea that gives excellent performance and is beginning to supplant the use of neural nets. A readable introduction is in Cristianini and Shawe-Taylor (2000).

支持向量机也可以用于在其他领域(例如，回归) 提供准确的预测。这是一个令人兴奋的想法，它提供了出色的性能，并开始取代神经网络的使用。一本可读性强的介绍书是 Cristianini and shaw taylor(2000)。

## 11. INFORMATION FROM A BLACK BOX
## 11. 来自黑匣子的信息

The dilemma posed in the last section is that the models that best emulate nature in terms of predictive accuracy are also the most complex and inscrutable. But this dilemma can be resolved by realizing the wrong question is being asked. Nature forms the outputs $\mathbf{y}$ from the inputs $\mathbf{x}$ by means of a black box with complex and unknown interior.

最后一节提出的难题是，在预测准确性方面最能模拟自然的模型也是最复杂和最不可思议的。但是，如果意识到所提出的问题是错误的，就可以解决这个困境。大自然通过一个内部复杂而未知的黑盒子，把输入 x 变成输出 y。

![](https://cdn.mathpix.com/cropped/0ff65c4b1d28187c8f7d0addfa01c783-11.jpg?height=66&width=301&top_left_y=916&top_left_x=746)

Current accurate prediction methods are also complex black boxes.
目前的准确预测方法也是复杂的黑盒子。

![](https://cdn.mathpix.com/cropped/0ff65c4b1d28187c8f7d0addfa01c783-11.jpg?height=78&width=312&top_left_y=1047&top_left_x=735)

So we are facing two black boxes, where ours seems only slightly less inscrutable than nature's. In data generated by medical experiments, ensembles of predictors can give cross-validated error rates significantly lower than logistic regression. My biostatistician friends tell me, "Doctors can interpret logistic regression." There is no way they can interpret a black box containing fifty trees hooked together. In a choice between accuracy and interpretability, they'll go for interpretability.

因此，我们面临着两个黑盒子，我们的黑盒子似乎只比自然界的黑盒子略少一些神秘。在医学实验生成的数据中，综合预测器可以给出比逻辑回归更低的交叉验证错误率。我的生物统计学家朋友告诉我，“医生可以解释逻辑回归." 他们不可能解释一个包含 50 棵树连接在一起的黑匣子。在准确性和可解释性之间做出选择时，他们会选择可解释性。

Framing the question as the choice between accuracy and interpretability is an incorrect interpretation of what the goal of a statistical analysis is. The point of a model is to get useful information about the relation between the response and predictor variables. Interpretability is a way of getting information. But a model does not have to be simple to provide reliable information about the relation between predictor and response variables; neither does it have to be a data model.
把问题框定为准确性和可解释性之间的选择是对统计分析目标的错误解释。模型的目的是获取有关响应和预测变量之间关系的有用信息。可解释性是获取信息的一种方式。但模型并不一定要简单，才能提供预测器和反应变量之间关系的可靠信息;它也不必是一个数据模型。

- The goal is not interpretability, but accurate information.
•目标不是可解释性，而是准确的信息。

The following three examples illustrate this point. The first shows that random forests applied to a medical data set can give more reliable information about covariate strengths than logistic regression. The second shows that it can give interesting information that could not be revealed by a logistic regression. The third is an application to a microarray data where it is difficult to conceive of a data model that would uncover similar information.

下面的三个例子说明了这一点。第一项研究表明，随机森林应用于医疗数据集可以提供比逻辑回归更可靠的协变量强度信息。第二种方法表明，它可以提供逻辑回归无法揭示的有趣信息。第三个是微阵列数据的应用，在这个应用中很难设想一个数据模型来揭示类似的信息。

### 11.1 Example I: Variable Importance in a} Survival Data Set
### 11.1 例子 I: 生存数据集的变量重要性
The data set contains survival or nonsurvival of 155 hepatitis patients with 19 covariates. It is available at ftp.ics.uci.edu/pub/MachineLearningDatabases and was contributed by Gail Gong. The description is in a file called hepatitis.names. The data set has been previously analyzed by Diaconis and Efron (1983), and Cestnik, Konenenko and Bratko (1987). The lowest reported error rate to date, $17 \%$, is in the latter paper.

该数据集包含155名肝炎患者的生存或非生存期，共19个协变量。它可以在ftp.ics.uci.edu/pub/MachineLearning-数据库上找到，由Gail Gong贡献。描述在一个叫肝炎名字的文件里。Diaconis 和 Efron(1983) 以及 Cestnik 、 Konenenko 和 Bratko(1987)之前对数据集进行了分析。迄今为止报道的最低错误率为17%，出现在后一篇论文中。

Diaconis and Efron refer to work by Peter Gregory of the Stanford Medical School who analyzed this data and concluded that the important variables were numbers $6,12,14,19$ and reports an estimated $20 \%$ predictive accuracy. The variables were reduced in two stages-the first was by informal data analvsis. The second refers to a more formal (unspecified) statistical procedure which I assume was logistic regression.

Diaconis 和 Efron 引用了斯坦福医学院的 Peter Gregory 的研究，他分析了这些数据，得出了重要变量是数字 6、12、14、19 的结论，并报告了估计 20%的预测准确率。变量的减少分两个阶段进行——第一个阶段是通过非正式的数据分析。第二种是指比较正式的(未指明)统计过程，我认为是逻辑回归。

Efron and Diaconis drew 500 bootstrap samples from the original data set and used a similar procedure to isolate the important variables in each bootstrapped data set. The authors comment, "Of the four variables originally selected not one was selected in more than 60 percent of the samples. Hence the variables identified in the original analysis cannot be taken too seriously." We will come back to this conclusion later.

Efron 和 Diaconis 从原始数据集中抽取了500个bootstrap 样本，并使用类似的程序分离每个 bootstrap 数据集中的重要变量。作者评论道:“在最初选择的四个变量中，超过60%的样本没有选择一个变量。因此，对原始分析中确定的变量不能太认真。" 我们稍后会得出这个结论。

####  Logistic Regression
#### 逻辑回归

The predictive error rate for logistic regression on the hepatitis data set is $17.4 \%$. This was evaluated by doing 100 runs, each time leaving out a randomly selected $10 \%$ of the data as a test set, and then averaging over the test set errors.

logistic 回归对肝炎数据集的预测错误率为17.4%。这是通过执行 100 次运行来评估的，每次都将随机选择的10%的数据作为测试集，然后对测试集的错误进行平均。

Usually, the initial evaluation of which variables are important is based on examining the absolute values of the coefficients of the variables in the logistic regression divided by their standard deviations. Figure 1 is a plot of these values.

通常，对哪些变量重要的初始评估是基于检验logistic 回归中变量系数的绝对值除以它们的标准差。图 1 是这些值的图。

The conclusion from looking at the standardized coefficients is that variables 7 and 11 are the most important covariates. When logistic regression is run using only these two variables, the cross-validated error rate rises to $22.9 \%$. Another way to find important variables is to run a best subsets search which, for any value $k$, finds the subset of $k$ variables having lowest deviance.

通过观察标准化系数得出的结论是变量 7 和 11 是最重要的协变量。当只使用这两个变量运行逻辑回归时，交叉验证的错误率上升到 22.9%。另一种寻找重要变量的方法是对于任意 k 值运行最佳子集搜索，找到偏差最小的 k 个变量的子集。

This procedure raises the problems of instability and multiplicity of models (see Section 7.1). There are about 4,000 subsets containing four variables. Of these, there are almost certainly a substantial number that have deviance close to the minimum and give different pictures of what the underlying mechanism is.

这一过程提出了模型不稳定性和多样性的问题 (见第 7.1 节)。大约有 4000 个子集包含四个变量。其中，几乎可以肯定的是有相当数量的异常接近最小值，并给出了潜在机制的不同图像。

![](https://cdn.mathpix.com/cropped/0ff65c4b1d28187c8f7d0addfa01c783-12.jpg?height=337&width=728&top_left_y=1086&top_left_x=254)

FIG. 1. Standardized coefficients logistic regression. 
FIG.1 标准系数逻辑回归。

![](https://cdn.mathpix.com/cropped/0ff65c4b1d28187c8f7d0addfa01c783-13.jpg?height=338&width=728&top_left_y=85&top_left_x=254)

FIG. 2. Variable importance-random forest.
FIG.2. 变量重要－随机森林

####  Random Forests
#### 随机森林

The random forests predictive error rate, evalu. ated by averaging errors over 100 runs, each time leaving out $10 \%$ of the data as a test set, is $12.3 \%-$ almost a $30 \%$ reduction from the logistic regression error.

随机森林的预测错误率是12.3%——几乎比逻辑回归误差减少了30%，通过对100次运行的平均误差进行评估，每次都将10%的数据作为测试集。

Random forests consists of a large number of randomly constructed trees, each voting for a class Similar to bagging (Breiman, 1996), a bootstrap sample of the training set is used to construct each tree. A random selection of the input variables is searched to find the best split for each node.

随机森林由大量随机构造的树组成，每棵树为一个类投票。类似于 bagging【译者注：专业名词】(Breiman, 1996)，使用训练集的自举样本来构建每棵树。对输入变量的随机选择进行搜索，以找到每个节点的最佳分割。

To measure the importance of the mth variable, the values of the mth variable are randomly permuted in all of the cases left out in the current bootstrap sample. Then these cases are run down the current tree and their classification noted. At the end of a run consisting of growing many trees, the percent increase in misclassification rate due to noising up each variable is computed. This is the measure of variable importance that is shown in Figure $1 .$

为了衡量第 m 个变量的重要性，在当前 bootstrap 样本中所遗漏的所有情况下，随机排列第 m 个变量的值。然后在当前树中运行这些案例，并记录它们的分类。在一次由种植许多树组成的运行结束时，计算由于干扰每个变量而导致的误分类率的增加百分比。这是图 1 所示的变量重要性的度量。

Random forests singles out two variables, the 12th and the 17 th, as being important. As a verification both variables were run in random forests, individually and together. The test set error rates over 100 replications were $14.3 \%$ each. Running both together did no better. We conclude that virtually all of the predictive capability is provided by a single variable, either 12 or 17 .

随机森林选出了两个重要的变量，第 12 个和第 17 个。作为验证，这两个变量分别或一起在随机森林中运行。超过100次重复的测试设置错误率为 14.3%。两者同时运行也没有更好的结果。我们的结论是几乎所有的预测能力是由一个单一的变量提供的　12 或 17。

To explore the interaction between 12 and 17 a bit further, at the end of a random forest run using all variables, the output includes the estimated value of the probability of each class vs. the case number. This information is used to get plots of the variable values (normalized to mean zero and standard deviation one) vs. the probability of death. The variable values are smoothed using a weighted linear regression smoother. The results are in Figure 3 for variables 12 and 17 .

为了进一步探索 12 和 17 之间的相互作用，在使用所有变量的随机森林运行的最后，输出包括每个类的概率与案例数的估计值。该信息用于获得变量值(归一化为均值 0 和标准差 1)与死亡概率的图。变量值用加权线性回归平滑器平滑。变量 12 和 17 的结果如图 3 所示。

![](https://cdn.mathpix.com/cropped/0ff65c4b1d28187c8f7d0addfa01c783-13.jpg?height=362&width=754&top_left_y=1060&top_left_x=241)

FIG. 3. Variable 17 vs. probability #1. 

![](https://cdn.mathpix.com/cropped/0ff65c4b1d28187c8f7d0addfa01c783-14.jpg?height=327&width=743&top_left_y=83&top_left_x=252)

Fig. 4. Variable importance-Bupa data.

The graphs of the variable values vs. class death probability are almost linear and similar. The two variables turn out to be highly correlated. Thinking that this might have affected the logistic regression results, it was run again with one or the other of these two variables deleted. There was little change.

变量值与阶级死亡概率的曲线图几乎是线性的和相似的。这两个变量是高度相关的。考虑到这可能会影响逻辑回归结果，删除这两个变量中的一个或另一个再次运行，几乎没有什么变化。

Out of curiosity, I evaluated variable importance in logistic regression in the same way that I did in random forests, by permuting variable values in the $10 \%$ test set and computing how much that increased the test set error. Not much helpvariables 12 and 17 were not among the 3 variables ranked as most important. In partial verification of the importance of 12 and 17 , I tried them separately as single variables in logistic regression. Variable 12 gave a $15.7 \%$ error rate, variable 17 came in at $19.3 \%$.

出于好奇，我在逻辑回归中评估变量重要性的方法与在随机森林中相同，通过排列 10%测试集中的变量值并计算这会增加多少测试集误差。没有多大帮助——变量 12 和 17 不在3个最重要的变量中。在部分验证 12 和 17 的重要性时，我将它们分别作为单变量在logistic回归中进行了尝试。变量 12 的错误率为15.7%，变量17的错误率为19.3%。

To go back to the original Diaconis-Efron analysis, the problem is clear. Variables 12 and 17 are surrogates for each other. If one of them appears important in a model built on a bootstrap sample, the other does not. So each one's frequency of occurrence is automatically less than $50 \%$. The paper lists the variables selected in ten of the samples. Either 12 or 17 appear in seven of the ten.

回到最初的Diaconis-Efron【译者注：专业名词】分析，问题很明显。变量 12 和 17 互为代理。如果其中一个在建立在引导样本上的模型中显得很重要， 那么另一个则不重要。每一个的出现频率自动小于50%本文列出了从 10 个样本中选择的变量。12 个或 17 个在 10 个中有 7 个出现。

### 11.2 Example II Clustering in Medical Data
### 11.2 例二医疗数据的聚类

The Bupa liver data set is a two-class biomedical data set also available at ftp.ics.uci.edu/pub/MachineLearningDatabases. The covariates are:

保柏肝脏数据集是一个两类生物医学数据集，也可在 ftp.ics.uci.edu/pub/MachineLearningDatabases上获得。协变量是:

1. mcv

mean corpuscular volume
指红细胞体积

2. alkphos

alkaline phosphotase
碱性 磷酸梅

3. sgpt

alamine aminotransferase
阿拉明转氨酶

4. sgot

aspartate aminotransferase
天冬氨酸转氨酶

5. gammagt

gamma-glutamyl transpeptidase
转肽酶

6. drinks

half-pint equivalents of alcoholic
喝相当于半品脱的酒精饮料
beverage drunk per day
每天喝的饮料

The first five attributes are the results of blood tests thought to be related to liver functioning. The 345 patients are classified into two classes by the severity of their liver malfunctioning. Class two is severe malfunctioning. In a random forests run,

前五个属性是血液测试的结果，被认为与肝脏功能有关。345名患者根据肝功能障碍的严重程度分为两类。二类是严重故障。在随机的森林里，误分类错误率为 28%。随机森林给出的变量重要性如图 4 所示。


![](https://cdn.mathpix.com/cropped/0ff65c4b1d28187c8f7d0addfa01c783-14.jpg?height=363&width=740&top_left_y=1060&top_left_x=254)

FIG. 5. Cluster averages-Bupa data. the misclassification error rate is $28 \%$. The variable importance given by random forests is in Figure 4 .

血液测试 3 和 5 是最重要的，其次是测试 4。随 机森林还输出一种内在相似性测度，可用于聚类。当应用此方法时，在第二类中发现了两个集群。每个变量的平均值被计算出来，并绘制在图 5 中的每个集群中。

Blood tests 3 and 5 are the most important, followed by test $4 .$ Random forests also outputs an intrinsic similarity measure which can be used to cluster. When this was applied, two clusters were discovered in class two. The average of each variable is computed and plotted in each of these clusters in Figure 5 .

血液测试 3 和 5 是最重要的，其次是测试 4。随 机森林还输出一种内在相似性测度，可用于聚类。当应用此方法时，在第二类中发现了两个集群。每个变量的平均值被计算出来，并绘制在图 5 中 的每个集群中。

An interesting facet emerges. The class two subjects consist of two distinct groups: those that have high scores on blood tests 3,4 , and 5 and those that have low scores on those tests.

一个有趣的方面出现了。第 2 类受试者由两个不同的组组成:在血液测试 3、4 和 5 中得分高的组和在这些测试中得分低的组。

### 11.3 Example III: Microarray Data

### 11.3 样例 III: 微阵列数据
Random forests was run on a microarray lymphoma data set with three classes, sample size of 81 and 4,682 variables (genes) without any variable selection [for more information about this data set, see Dudoit, Fridlyand and Speed, (2000)]. The error rate was low. What was also interesting from a scientific viewpoint was an estimate of the importance of each of the 4,682 gene expressions.

随机森林是在一个微阵列淋巴瘤数据集上运行的，有三个类别，样本大小为 81 和 4682 个变量(基因)，没有任何变量选择[有关这个数据集的更多信息，参见 Dudoit, Fridlyand and Speed，(2000)]。错误率低。从科学的观点来看，同样有趣的是对 4682 个基因表达中每一个的重要性的估计。

The graph in Figure 6 was produced by a run of random forests. This result is consistent with assessments of variable importance made using other algorithmic methods, but appears to have sharper detail.

图 6 中的图是由一系列随机森林生成的。这一结果与使用其他算法方法进行的变量重要性评估一致，但似乎有更清晰的细节。

### 11.4 Remarks about the Examples
### 11.4 关于例子的评论

The examples show that much information is available from an algorithmic model. Friedman (1999) derives similar variable information from a different way of constructing a forest. The similarity is that they are both built as ways to give low predictive error.

实例表明，从算法模型中可以获得大量的信息。弗里德曼(1999)从构建森林的不同方式获得类似的可变信息。相似之处在于，它们都是为了降低预测误差而设计的。

There are 32 deaths and 123 survivors in the hepatitis data set. Calling everyone a survivor gives a baseline error rate of $20.6 \%$. Logistic regression lowers this to $17.4 \%$. It is not extracting much useful information from the data, which may explain its inability to find the important variables. Its weakness might have been unknown and the variable importances accepted at face value if its predictive accuracy was not evaluated.

在肝炎数据集中，有 32 人死亡，123 人存活。称每个人都是幸存者的基线错误率为 20.6% 。Logistic 回归将其降低到 17.4%。它没有从数据中提取出很多有用的信息，这可能解释了它无法找到重要的变量。它的弱点可能是未知的，如果它的预测准确性没有被评估，那么变量的重要性就会被接受。

Random forests is also capable of discovering important aspects of the data that standard data models cannot uncover. The potentially interesting clustering of class two patients in Example II is an illustration. The standard procedure when fitting data models such as logistic regression is to delete variables; to quote from Diaconis and Efron (1983) again, "...statistical experience suggests that it is unwise to fit a model that depends on 19 variables with only 155 data points available." Newer methods in machine learning thrive on variables-the more the better. For instance, random forests does not overfit. It gives excellent accuracy on the lymphoma data set of Example III which has over 4,600 variables, with no variable deletion and is capable of extracting variable importance information from the data.

随机森林还能够发现标准数据模型无法发现的数 据的重要方面。例 II 中可能有趣的二类患者群集就是一个例证。拟合数据模型(如 logistic 回归)的标准程序是删除变量;再次引用迪亚康尼斯和埃夫隆(1983)的话，“....统计经验表明， 拟合一个依赖于 19 个变量、仅有 155 个数据点的模型是不明智的。机器学习中更新的方法依赖于变量——变量越多越好。例如，随机森林不会过度拟合。它对例 III 的淋巴瘤数据集具有极好的准确性，该数据集有超过 4600 个变量，没有变量删除，并能够从数据中提取变量重要信息。

![](https://cdn.mathpix.com/cropped/0ff65c4b1d28187c8f7d0addfa01c783-15.jpg?height=494&width=754&top_left_y=929&top_left_x=241)

FIG. 6. Microarray variable importance. 
图６.　微阵列变量的重要性

These examples illustrate the following points:
这些例子说明了以下几点:

- Higher predictive accuracy is associated with more reliable information about the underlying data mechanism. Weak predictive accuracy can lead to questionable conclusions.

- 更高的预测准确性与更可靠的基础数据机制信息相关。预测准确性差可能导致可疑的结论。

- Algorithmic models can give better predictive accuracy than data models, and provide better information about the underlying mechanism.

- 算法模型可以提供比数据模型更好的预测精度，并提供关于底层机制的更好的信息。

## 12. FINAL REMARKS
## 12.　最后的附注

The goals in statistics are to use data to predict and to get information about the underlying data mechanism. Nowhere is it written on a stone tablet what kind of model should be used to solve problems involving data. To make my position clear, I am not against data models per se. In some situations they are the most appropriate way to solve the problem. But the emphasis needs to be on the problem and on the data.

统计的目标是使用数据来预测和获取关于底层数据机制的信息。没有哪块石碑上写着用什么样的模型来解决涉及数据的问题。为了表明我的立场，我本身并不反对数据模型。在某些情况下，它们是最合适的解决问题的方法。但重点应该放在问题和数据上。

Unfortunately, our field has a vested interest in data models, come hell or high water. For instance, see Dempster's (1998) paper on modeling. His position on the 1990 Census adjustment controversy is particularly interesting. He admits that he doesn't know much about the data or the details, but argues that the problem can be solved by a strong dose of modeling. That more modeling can make errorridden data accurate seems highly unlikely to me.

不幸的是，无论发生什么情况，我们的领域都对数据模型感兴趣。例如，参见 Dempster(1998)关于建模的论文。他对1990年人口普查调整争议的立场特别有趣。他承认自己对数据和细节了解不多，但他认为这个问题可以通过大量的建模来解决。在我看来，更多的建模可以使充满错误的数据变得精确是极不可能的。

Terrabytes of data are pouring into computers from many sources, both scientific, and commercial, and there is a need to analyze and understand the data. For instance, data is being generated at an awesome rate by telescopes and radio telescopes scanning the skies. Images containing millions of stellar objects are stored on tape or disk. Astronomers need automated ways to scan their data to find certain types of stellar objects or novel objects. This is a fascinating enterprise, and I doubt if data models are applicable. Yet I would enter this in my ledger as a statistical problem.

千兆字节的数据从科学和商业的许多来源涌入计算机，有必要分析和理解这些数据。例如，扫描天空的望远镜和射电望远镜正在以惊人的速度生成数据。包含数百万颗恒星的图像存储在磁带或磁盘上。天文学家需要自动化的方法来扫描他
们的数据，以发现某些类型的恒星物体或新物体。这是一个迷人的企业，我怀疑数据模型是否适用。然而，我会把它作为一个统计问题记入我的账簿。

The analysis of genetic data is one of the most challenging and interesting statistical problems around. Microarray data, like that analyzed in Section $11.3$ can lead to significant advances in understanding genetic effects. But the analysis of variable importance in Section $11.3$ would be difficult to do accurately using a stochastic data model.

遗传数据的分析是最具挑战性和最有趣的统计问题之一。像第 11.3 节中分析的那样，微阵列数据可以在理解遗传效应方面取得重大进展。但是，第 11.3 节中关于变量重要性的分析很难使用随机数据模型来精确地进行。

Problems such as stellar recognition or analysis of gene expression data could be high adventure for statisticians. But it requires that they focus on solving the problem instead of asking what data model they can create. The best solution could be an algorithmic model, or maybe a data model, or maybe a combination. But the trick to being a scientist is to be open to using a wide variety of tools.

对统计学家来说，恒星识别或基因表达数据分析等问题可能是高风险的。但这要求他们专注于解决问题，而不是问他们可以创建什么数据模型。最好的解决方案可能是一个算法模型，或者一个数据模型，或者组合。但成为科学家的诀窍是要对使用各种各样的工具持开放态度。

The roots of statistics, as in science, lie in working with data and checking theory against data. I hope in this century our field will return to its roots. There are signs that this hope is not illusory. Over the last ten years, there has been a noticeable move toward statistical work on real world problems and reaching out by statisticians toward collaborative work with other disciplines. I believe this trend will continue and, in fact, has to continue if we are to survive as an energetic and creative field.

与科学一样，统计学的根源在于运用数据，并用数据检验理论。我希望在本世纪， 我们的领域将回归其根源。有迹象表明，这种希望并非虚幻。在过去的十年里，有一个明显的趋势，即统计学家对现实世界问题的统计工作，以及与其他学科的合作工作。我相信，如果我们要作为一个充满活力和创造力的领域生存下去，这种趋势将会继续，事实上必须继续。

## GLOSSARY
## 词汇表

Since some of the terms used in this paper may not be familiar to all statisticians, I append some definitions.

由于本文中使用的一些术语可能不是所有统计学家都熟悉的，我在此附加一些定义。

Infinite test set error. Assume a loss function $L(y, \hat{y})$ that is a measure of the error when $y$ is the true response and $\hat{y}$ the predicted response. In classification, the usual loss is 1 if $y \neq \hat{y}$ and zero if $y=\hat{y}$. In regression, the usual loss is $(y-\hat{y})^{2}$. Given a set of data (training set) consisting of $\left\{\left(y_{n}, \mathbf{x}_{n}\right) n=1,2, \ldots, N\right\}$, use it to construct a predictor function $\phi(\mathbf{x})$ of $y$. Assume that the training set is i.i.d drawn from the distribution of the random vector $Y, \mathbf{X}$. The infinite test set error is $E(L(Y, \phi(\mathbf{X})))$. This is called the generalization error in machine learning.

无限测试集错误。假设损失函数是 $L(y, \hat{y})$是误差的度量，y 是真实响应 yˆ是预测响应。分类时，如果 y = yˆ，通常损失为 1，如果 y = y，通常损失为 0。在回归中，通常的损失是$(y-\hat{y})^{2}$。 给定一组数据(训练集)，由$\left\{\left(y_{n}, \mathbf{x}_{n}\right) n=1,2, \ldots, N\right\}$，用它来构造预测函数$\phi(\mathbf{x})$假设训练集为 i。d 从随机向量 $Y, \mathbf{X}$的分布中得到，无限的测试集误差为 $E(L(Y, \phi(\mathbf{X})))$。这被称为机器学习中的泛化误差。

The generalization error is estimated either by setting aside a part of the data as a test set or by cross-validation.
泛化误差是通过将一部分数据作为测试集或交叉验证来估计的。

Predictive accuracy. This refers to the size of the estimated generalization error. Good predictive accuracy means low estimated error.
预测精度。 这是指估计的泛化误差的大小。 好的预测精度意味着低的估计误差。

Trees and nodes. This terminology refers to decision trees as described in the Breiman et al book (1984).
树木和节点。这个术语指的是 Breiman 等人的书(1984)中描述的决策树。

Dropping an $\mathbf{x}$ down a tree. When a vector of predictor variables is "dropped" down a tree, at each intermediate node it has instructions whether to go left or right depending on the coordinates of $\mathbf{x}$. It stops at a terminal node and is assigned the prediction given by that node.

从树上扔下 x 。当一个包含预测变量的向量被“拖下”树时，在每个中间节点上，它都有根据 x坐标向左或向右移动的指令。它停在一个终端节点上，并被分配到该节点给出的预测。

Bagging. An acronym for "bootstrap aggregating." Start with an algorithm such that given any training set, the algorithm produces a prediction function $\phi(\mathbf{x})$. The algorithm can be a decision tree construction, logistic regression with variable deletion, etc. Take a bootstrap sample from the training set and use this bootstrap training set to construct the predictor $\phi_{1}(\mathbf{x})$. Take another bootstrap sample and using this second training set construct the predictor $\phi_{2}(\mathbf{x})$. Continue this way for $K$ steps. In regression, average all of the $\left\{\phi_{k}(\mathbf{x})\right\}$ to get the bagged predictor at $\mathbf{x}$. In classification, that class which has the plurality vote of the $\left\{\phi_{k}(\mathbf{x})\right\}$ is the bagged predictor. Bagging has been shown effective in variance reduction (Breiman, 1996b).

装袋。“引导聚合”的首字母缩写。“从这样一个算法开始，给定任何训练集，该算法产生一个预测函数 $\phi(\mathbf{x})$。 该算法可以是构造决策树、删除变量的 logistic 回归等。 从训练集中选取一个自举样本，利用该自举训练集构造预测器$\phi_{1}(\mathbf{x})$。取另一个 bootstrap 样本，使用第二个训练集构造预测器$\phi_{2}(\mathbf{x})$。以这种方式进行 K 步。在回归中，平均所有的$\left\{\phi_{k}(\mathbf{x})\right\}$得到在分类中，具有 φ 多数票的类别 $\left\{\phi_{k}(\mathbf{x})\right\}$ 是袋装预测器。套袋已被证明在方差减少方面有效(Breiman, 1996b)。

Boosting. This is a more complex way of forming an ensemble of predictors in classification than bagging (Freund and Schapire, 1996). It uses no randomization but proceeds by altering the weights on the training set. Its performance in terms of low prediction error is excellent (for details see Breiman, 1998).

Boosting【译者注：专业名词】。 与套袋法相比，这是一种更复杂的方式来形成分类中的预测因子集合(Freund and Schapire, 1996)。它不使用随机化，而是通过改变训练集中的权重来进行。它在低预测误差方面的表现非常出色(详情见 Breiman, 1998)。

## ACKNOWLEDGMENTS
## 致谢

Many of my ideas about data modeling were formed in three decades of conversations with my old friend and collaborator, Jerome Friedman. Conversations with Richard Olshen about the Cox model and its use in biostatistics helped me to understand the background. I am also indebted to William Meisel, who headed some of the prediction projects I consulted on and helped me make the transition from probability theory to algorithms, and to Charles Stone for illuminating conversations about the nature of statistics and science. I'm grateful also for the comments of the editor, Leon Gleser, which prompted a major rewrite of the first draft of this manuscript and resulted in a different and better paper.

我的许多关于数据建模的想法都是在与我的老朋友兼合作者杰罗姆·弗里德曼(Jerome Friedman) 30 年的对话中形成的。与Richard Olshen 关于 Cox 模型及其在生物统计学中的应用的对话帮助我了解了背景。我还感谢威廉·梅塞尔(William Meisel)，他领导了我参与的一些预测项目，帮助我从概率论过渡到算法。我还感谢查尔斯·斯通 (Charles Stone)，他与我就统计学和科学的本质进行了富有启蒙性的对话。我也感谢编辑 Leon Gleser 的评论，正是他的评论促使我对这篇手稿的初稿进行了重大的重写，最终写出了一篇不同的、更好的论文。

## REFERENCES
## 参考文献

AMIT, Y. and GEMAN, D. (1997). Shape quantization and recognition with randomized trees. Neural Computation $\mathbf{9} 1545$ $1588 .$

ARENA, C., SUSSMAN, N., CHIANG, K., MAZUMDAR, S., MACINA, O. and LI, W. (2000). Bagging Structure-Activity Relationships: A simulation study for assessing misclassification rates. Presented at the Second Indo-U.S. Workshop on Mathematical Chemistry, Duluth, MI. (Available at NSussman@server.ceoh.pitt.edu).

BICKEL, P., RITOV, Y. and STOKER, T. (2001). Tailor-made tests for goodness of fit for semiparametric hypotheses. Unpublished manuscript.

BREIMAN, L. (1996a). The heuristics of instability in model selection. Ann. Statist. $\mathbf{2 4}$ 2350-2381.

BREIMAN, L. (1996b). Bagging predictors. Machine Learning $J .$ $26123-140$

BREIMAN, L. (1998). Arcing classifiers. Discussion paper, Ann Statist. $\mathbf{2 6} 801-824 .$

BreIMAN. L. (2000). Some infinity theory for tree ensembles. (Available at www.stat.berkeley.edu/technical reports).

BREIMAN, L. (2001). Random forests. Machine Learning J. $\mathbf{4 5} 5$ 32

BREIMAN, L. and FRIEDMAN, J. (1985). Estimating optimal transformations in multiple regression and correlation. J. Amer. Statist. Assoc. $\mathbf{8 0}$ 580-619.

BREIMAN, L., FRIEDMAN, J., OLSHEN, R. and STONE, C. (1984). Classification and Regression Trees. Wadsworth, Belmont, CA. CRISTIANINI, N. and SHAWE-TAYLOR, J. (2000). An Introduction to Support Vector Machines. Cambridge Univ. Press.

DANIEL, C. and Wood, F. (1971). Fitting equations to data. Wiley, New York.

DEMPSTER, A. (1998). Logicist statistic $1 .$ Models and Modeling. Statist. Sci. $\mathbf{1 3} 3$ 248-276.

DIACONIS, P. and EFRON, B. (1983). Computer intensive methods in statistics. Scientific American $\mathbf{2 4 8}$ 116-131.

DominGos, P. (1998). Occam's two razors: the sharp and the blunt. In Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining (R. Agrawal and P. Stolorz, eds.) 37-43. AAAI Press, Menlo Park, CA.

DominGos, P. (1999). The role of Occam's razor in knowledge discovery. Data Mining and Knowledge Discovery 3 409-425.

Dudort, S., FrIDLYAND, J. and SPEED, T. (2000). Comparison of discrimination methods for the classification of tumors. (Available at www.stat.berkeley.edu/technical reports).

FREEDMAN, D. (1987). As others see us: a case study in path analysis (with discussion). J. Ed. Statist. $\mathbf{1 2}$ 101-223.

FREEDMAN, D. (1991). Statistical models and shoe leather. Sociological Methodology 1991 (with discussion) 291-358.

FREEDMAN, D. (1991). Some issues in the foundations of statistics. Foundations of Science $\mathbf{1}$ 19-83.

FREEDMAN, D. (1994). From association to causation via regression. Adv. in Appl. Math. $\mathbf{1 8} 59-110 .$

FREUND, Y. and SCHAPIRE, R. (1996). Experiments with a new boosting algorithm. In Machine Learning: Proceedings of the Thirteenth International Conference 148-156. Morgan Kaufmann, San Francisco.

FriedMan, J. (1999). Greedy predictive approximation: a gradient boosting machine. Technical report, Dept. Statistics Stanford Univ.

FRIEDMAN, J., HASTIE, T. and TIBSHIRANI, R. (2000). Additive logistic regression: a statistical view of boosting. Ann. Statist. $28337-407$

GIFI, A. (1990). Nonlinear Multivariate Analysis. Wiley, New York.

Ho, T. K. (1998). The random subspace method for constructing decision forests. IEEE Trans. Pattern Analysis and Machine Intelligence $\mathbf{2 0}$ 832-844.

LANDSWHER, J., PREIBON, D. and SHOEMAKER, A. (1984). Graphical methods for assessing logistic regression models (with discussion). J. Amer. Statist. Assoc. 79 61-83.

MCCULLAGH, P. and NELDER, J. (1989). Generalized Linear Models. Chapman and Hall, London.

MEISEL, W. (1972). Computer-Oriented Approaches to Pattern Recognition. Academic Press, New York.

MICHIE, D., SPIEGELHALTER, D. and TAYLOR, C. (1994). Machine Learning, Neural and Statistical Classification. Ellis Horwood, New York.

MOSTELLER, F. and TUKEY, J. (1977). Data Analysis and Regression. Addison-Wesley, Redding, MA.

MoUNTAIN, D. and HsIAO, C. (1989). A combined structural and flexible functional approach for modelenery substitution. J. Amer. Statist. Assoc. $\mathbf{8 4}$ 76-87.

STONE, M. (1974). Cross-validatory choice and assessment of statistical predictions. J. Roy. Statist. Soc. B $\mathbf{3 6}$ 111-147.

VAPNIK, V. (1995). The Nature of Statistical Learning Theory. Springer, New York.

VAPNIK, V (1998). Statistical Learning Theory. Wiley, New York.

WAHBA, G. (1990). Spline Models for Observational Data. SIAM, Philadelphia.

ZHANG, H. and SINGER, B. (1999). Recursive Partitioning in the Health Sciences. Springer, New York.
